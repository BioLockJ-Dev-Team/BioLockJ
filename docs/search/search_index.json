{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is BioLockJ BioLockJ optimizes your bioinformatics pipeline and metagenomics analysis. Modular design logically partitions analysis and expedites failure recovery Automated script generation eliminates syntax errors and ensures uniform execution Standardized OTU abundance tables facilitate analysis across datasets Batch scripts take advantage of parallelization on the cluster job queue configuration file consolidates project details into a principal reference document (and can reproduce analysis) BioModule interface provides a flexible mechanism for adding new functionality BioLockJ User Guide: Getting Started Commands Pipeline Componenets the config file Properties Modules the metadata input files Dependencies Features Check Dependencies before pipeline start Failure Recovery Validation Supported Environments Expand BioLockJ by Building Modules BioLockJ API Examples and Templates Example Pipeline FAQ Links for Developers Javadocs https://BioLockJ-Dev-Team.github.io/BioLockJ/javadocs/ Developement tests in the sheepdog_testing_suite https://github.com/BioLockJ-Dev-Team/sheepdog_testing_suite The user guide for our latest stable version https://biolockj-dev-team.github.io/BioLockJ/ The user guide for the current development version, and previous stable versions https://biolockj.readthedocs.io/en/latest/ Guidelines for new modules Building Modules Citing BioLockJ If you use BioLockJ in your research, you should cite BioLockJ itself AND the tools that make up the pipeline. The majority of BioLockJ modules are wrappers for independent tools. See the summary of your pipeline for citation information from the modules in your pipeline. This information is also available in the modules' documentation. To cite BioLockJ itself, please cite the public project git repository (https://github.com/BioLockJ-Dev-Team/BioLockJ) and author Mike Sioda.","title":"Home"},{"location":"#what-is-biolockj","text":"BioLockJ optimizes your bioinformatics pipeline and metagenomics analysis. Modular design logically partitions analysis and expedites failure recovery Automated script generation eliminates syntax errors and ensures uniform execution Standardized OTU abundance tables facilitate analysis across datasets Batch scripts take advantage of parallelization on the cluster job queue configuration file consolidates project details into a principal reference document (and can reproduce analysis) BioModule interface provides a flexible mechanism for adding new functionality","title":"What is BioLockJ"},{"location":"#biolockj-user-guide","text":"Getting Started Commands Pipeline Componenets the config file Properties Modules the metadata input files Dependencies Features Check Dependencies before pipeline start Failure Recovery Validation Supported Environments Expand BioLockJ by Building Modules BioLockJ API Examples and Templates Example Pipeline FAQ","title":"BioLockJ User Guide:"},{"location":"#links-for-developers","text":"Javadocs https://BioLockJ-Dev-Team.github.io/BioLockJ/javadocs/ Developement tests in the sheepdog_testing_suite https://github.com/BioLockJ-Dev-Team/sheepdog_testing_suite The user guide for our latest stable version https://biolockj-dev-team.github.io/BioLockJ/ The user guide for the current development version, and previous stable versions https://biolockj.readthedocs.io/en/latest/ Guidelines for new modules Building Modules","title":"Links for Developers"},{"location":"#citing-biolockj","text":"If you use BioLockJ in your research, you should cite BioLockJ itself AND the tools that make up the pipeline. The majority of BioLockJ modules are wrappers for independent tools. See the summary of your pipeline for citation information from the modules in your pipeline. This information is also available in the modules' documentation. To cite BioLockJ itself, please cite the public project git repository (https://github.com/BioLockJ-Dev-Team/BioLockJ) and author Mike Sioda.","title":"Citing BioLockJ"},{"location":"BioLockJ-Api/","text":"BioLockJ API BioLockJ comes with an API. For the most up-to-date information about how to use the API, see the help menu: biolockj_api help BioLockJ API v1.2.8 - UNCC Fodor Lab Usage: biolockj-api <query> [options...] For some uses, redirecting stderr is recommended: biolockj-api <query> [options...] 2> /dev/null Use biolockj-api without args to get help menu. Options: Options shown in [ ] are optional for a given query. --external-modules <dir> path to a directory containing additional modules --module <module_path> class path for a specific module --property <property> a specific property --value <value> a vlue to use for a specific property --config <file> file path for a configuration file giving one or more property values --verbose flag indicating that all messages should go to standard err, including some that are typically disabled. query: listModules [ --external-modules <dir> ] Returns a list of classpaths to the classes that extend BioModule. listApiModules [--external-modules <dir> ] Like listModules but limit list to modules that implement the ApiModule interface. listProps [ --module <module_path> ] Returns a list of properties. If no args, it returns the list of properties used by the BioLockJ backbone. If a modules is given, then it returns a list of all properties used by that module. listAllProps [ --external-modules <dir> ] Returns a list of all properties, include all backbone properties and all module properties. Optionally supply the path to a directory containing additional modules to include their properties. propType --property <property> [ --module <module_path> [ --external-modules <dir> ] ] Returns the type expected for the property: String, list, integer, positive number, etc. If a module is supplied, then the modules propType method is used. describeProp --property <property> [ --module <module_path> [ --external-modules <dir> ] ] Returns a description of the property. If a module is supplied, then the modules getDescription method is used. propValue --property <property> [ --config <file> ] Returns the value for that property given that config file (optional) or no config file (ie the default value) isValidProp --property <property> --value <value> [ --module <module_path> [--external-modules <dir>] ] T/F/NA. Returns true if the value (val) for the property (prop) is valid; false if prop is a property but val is not a valid value, and NA if prop is not a recognized property. IF a module is supplied, then additionally call the validateProp(key, value) for that module, or for EACH module if a comma-separated list is given. propInfo Returns a json formatted list of the general properties (listProps) with the type, descrption and default for each property moduleInfo [--external-modules <dir>] Returns a json formatted list of all modules and for each module that implements the ApiModule interface, it lists the props used by the module, and for each prop the type, descrption and default. help (or no args) Print help menu. listModules and listApiModules are nearly identical. The methods that allow the API to interface with modules are in the ApiModule interface, not all BioModules implement that interface. Once all of the build-in modules have those methods, then these two functions will be identical; the BioModule interface will absorb the ApiModule interface, and listApiModules will be depricated. listAllProps is the union of all possible output from listProps . propInfo returns information equivelent to calling biolockj_api listProps and creating a for-loop to call biolockj_api propType $PROP , biolockj_api describeProp $PROP and biolockj_api propValue $PROP for each PROP in the list. moduleInfo returns information equivelent to calling biolockj_api listModules and creating a for-loop to call biolockj_api listProps $MODULE and for each of its properties,","title":"BioLockJ API"},{"location":"BioLockJ-Api/#biolockj-api","text":"BioLockJ comes with an API. For the most up-to-date information about how to use the API, see the help menu: biolockj_api help BioLockJ API v1.2.8 - UNCC Fodor Lab Usage: biolockj-api <query> [options...] For some uses, redirecting stderr is recommended: biolockj-api <query> [options...] 2> /dev/null Use biolockj-api without args to get help menu. Options: Options shown in [ ] are optional for a given query. --external-modules <dir> path to a directory containing additional modules --module <module_path> class path for a specific module --property <property> a specific property --value <value> a vlue to use for a specific property --config <file> file path for a configuration file giving one or more property values --verbose flag indicating that all messages should go to standard err, including some that are typically disabled. query: listModules [ --external-modules <dir> ] Returns a list of classpaths to the classes that extend BioModule. listApiModules [--external-modules <dir> ] Like listModules but limit list to modules that implement the ApiModule interface. listProps [ --module <module_path> ] Returns a list of properties. If no args, it returns the list of properties used by the BioLockJ backbone. If a modules is given, then it returns a list of all properties used by that module. listAllProps [ --external-modules <dir> ] Returns a list of all properties, include all backbone properties and all module properties. Optionally supply the path to a directory containing additional modules to include their properties. propType --property <property> [ --module <module_path> [ --external-modules <dir> ] ] Returns the type expected for the property: String, list, integer, positive number, etc. If a module is supplied, then the modules propType method is used. describeProp --property <property> [ --module <module_path> [ --external-modules <dir> ] ] Returns a description of the property. If a module is supplied, then the modules getDescription method is used. propValue --property <property> [ --config <file> ] Returns the value for that property given that config file (optional) or no config file (ie the default value) isValidProp --property <property> --value <value> [ --module <module_path> [--external-modules <dir>] ] T/F/NA. Returns true if the value (val) for the property (prop) is valid; false if prop is a property but val is not a valid value, and NA if prop is not a recognized property. IF a module is supplied, then additionally call the validateProp(key, value) for that module, or for EACH module if a comma-separated list is given. propInfo Returns a json formatted list of the general properties (listProps) with the type, descrption and default for each property moduleInfo [--external-modules <dir>] Returns a json formatted list of all modules and for each module that implements the ApiModule interface, it lists the props used by the module, and for each prop the type, descrption and default. help (or no args) Print help menu. listModules and listApiModules are nearly identical. The methods that allow the API to interface with modules are in the ApiModule interface, not all BioModules implement that interface. Once all of the build-in modules have those methods, then these two functions will be identical; the BioModule interface will absorb the ApiModule interface, and listApiModules will be depricated. listAllProps is the union of all possible output from listProps . propInfo returns information equivelent to calling biolockj_api listProps and creating a for-loop to call biolockj_api propType $PROP , biolockj_api describeProp $PROP and biolockj_api propValue $PROP for each PROP in the list. moduleInfo returns information equivelent to calling biolockj_api listModules and creating a for-loop to call biolockj_api listProps $MODULE and for each of its properties,","title":"BioLockJ API"},{"location":"Building-Modules/","text":"Building New Modules Any Java class that implements the BioModule interface can be added to a BioLockJ pipeline. The BioLockJ v1.0 implementation is currently focused on metagenomics analysis, but the generalized application framework is not limited to this domain. Users can implement new BioModules to automate a wide variety of bioinformatics and report analytics. The BioModule interface was designed so that users can develop new modules on their own. Beginners See the BioModule hello world tutorial. Coding your module To create a new BioModule , simply extend one of the abstract Java superclasses, code it's abstract methods, and add it to your pipeline with #BioModule tag your Config file: BioModuleImpl : Extend if a more specific interface does not apply ScriptModuleImpl : Extend if your module generates and executes bash scripts JavaModuleImpl : Extend if your module only runs Java code ClassifierModuleImpl : Extend to support a new classifier program ParserModuleImpl : Extend to parse output of a new classifier program R_Module : Extend if your module generates and executes R scripts To support a new classifier, create 3 modules that implement the following interfaces: ClassifierModule : Implement to generate bash scripts needed to call classifier program ParserModule : Implement to parse classifier output, configure as classifier post-requisite OtuNode : Classifier specific implementation holds OTU information for 1 sequence BioModuleImpl is the top-level superclass for all modules. Method Description checkDependencies() Must override. Called before executeTask() to identify Configuration errors and perform runtime validations. executeTask() Must override. Executes core module logic. cleanUp() Called after executeTask() to run cleanup operations, update Config properties, etc. getInputFiles() Return previous module output. getModuleDir() Return module root directory. getOutputDir() Return module output directory. getPostRequisiteModules() Returns a list of BioModules to run after the current module. getPreRequisiteModules() Returns a list of BioModules to run before the current module. getSummary() Return output directory summary. Most modules override this method by adding module specific summary details to super.getSummary(). getTempDir() Return module temp directory. setModuleDir(path) Set module directory. ScriptModuleImpl extends BioModuleImpl : superclass for script-generating modules. Method Description buildScript(files) Must override. Called by executeTask() for datasets with forward reads only. The return type is a list of lists. Each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from getInputFiles(). buildScriptForPairedReads(files) Calls back to buildScript(files) by default. Subclasses override this method to generate unique scripts for datasets containing paired reads. checkDependencies() Called before executeTask() to validate script.batchSize , script.exitOnError , script.numThreads , script.permissions , script.timeout getJobParams() Return shell command to execute the MAIN script. getScriptDir() Return module script directory. getSummary() Adds the script directory summary to super.getSummary(). Most modules override this method by adding module specific summary details to super.getSummary(). getTimeout() Return script.timeout . getWorkerScriptFunctions() Return bash script lines for any functions needed in the worker scripts. JavaModuleImpl extends ScriptModuleImpl : superclass for pure Java modules. To avoid running code on the cluster head node, a temporary instance of BioLockJ is spawned on a cluster node which is launched by the sole worker script from the job queue. Method Description runModule() Must override. Executes core module logic. buildScript(files) This method returns a single line calling java on the BioLockJ source code, passing -d parameter to run in direct mode and the full class name of the JavaModule to indicate the module to run. getSource() Determines if running code from Jar or source code in order to write valid bash script lines. getTimeout() Return java.timeout . moduleComplete() Create the script success indicator file. moduleFailed() Create the script failures indicator file. ClassifierModuleImpl extends ScriptModuleImpl : biolockj.module.classifier superclass. Method Description buildScriptForPairedReads(files) Called by executeTask() for datasets with paired reads. The return type is a list of lists, where each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from SeqUtil .getPairedReads(getInputFiles()). checkDependencies() Validate Configuration properties exe.classifier and exe.classifierParams , verify sequence file format, log classifier version info, and verify no biolockj.module.seq modules are configured run after the ClassifierModule . Subclasses should call super.checkDependencies() if overriding this method to retain these verifications. executeTask() Call buildScript(files) or buildScriptForPairedReads(files) based input sequence format and calls BashScriptBuilder to generate the main script + 1 worker script for every script.batchSize samples. To change the batch scheme, override this method to call the alternate BashScriptBuilder .buildScripts() method signiture and hard code the batch size. All biolockj.module.classifier modules override this method. getClassifierExe() Return Configuration property exe.classifier to call the classifier program in the bash scripts. If the classifier is not included in cluster.modules , validate that value is a valid file path. If exe.classifier is undefined, replace the property prefix exe with the lowercase prefix of the module class name (less the standard module suffix classifier ). For example, use rdp.classifier for RdpClassifier and kraken.classifier for KrakenClassifier . This allows users to define all classifier programs in a default Configuration file rather than setting exe.clssifier in each project Configuration file. getClassifierParams() Return Configuration property exe.classifierParams which may contain a list of parameters (without hyphens) to pass to the classifier program in the bash scripts. If exe.classifierParams is undefined, replace the property prefix exe with the lowercase prefix of the module class name as described for exe.classifier . getSummary() Adds input directory summary to super.getSummary(). Most modules override this method to add module specific summary details to super.getSummary(). logVersion() Run exe.classifier --version to log version info. RDP overrides this method to return null since the version switch is not supported. ParserModuleImpl extends JavaModuleImpl : biolockj.module.implicit.parser superclass. Method Description parseSamples() Must override. Called by executeTask() to populate the Set returned by getParsedSamples(). Each classifier requires a unique parser module to decode its output. This method should iterate through the classifier reports to build OtuNode s for each sample-OTU found in the report. The OtuNode s are stored in a ParsedSample and cached via addParsedSample( ParsedSample ). addParsedSample( sample ) Add the ParsedSample to the Set returned by getParsedSamples(). buildOtuTables() Generate OTU abundance tables from ClassifierModule output. checkDependencies() Validate Configuration properties ( report.minOtuCount , report.minOtuThreshold , report.logBase ) and verify no biolockj.module.classifier modules are configured to run after the ParserModule . executeTask() If report.numHits =Y, add \"Num_Hits\" column to metadata containing the number of reads that map to any OTU for each sample. Calls buildOtuTables() to generate module output. getParsedSample(id) Return the ParsedSample from the the Set returned by getParsedSamples() for a given id. getParsedSamples() Return 1 ParsedSample for each classified sample in the dataset. OtuNodeImpl is the superclass for the biolockj.node package. Method Description addOtu(level, otu) A node represents a single OTU, each level in the taxonomic hierarchy is populated with this method. getCount() Get the OTU count. getLine() Get the classifier report line used to create the node. getOtuMap() This map may contain 1 element for each of the report.taxonomyLevels and is populated by addOtu(level, otu). getSampleId() Get the sample ID to which the OTU belongs. report() Print node info to log file as DEBUG line - not visible unless pipeline.logLevel=DEBUG . setCount(num) Set the OTU count. setLine(line) Set the classifier report line used to create the node. setSampleId(id) set the sample ID to which the OTU belongs. OtuNodeImpl methods do not need to be overridden. New OtuNode implementations should call existing methods from their constructor. Document your module The BioLockJ API allows outside resources to get information about the BioLockJ program and any available modules. To interface with the API, your module will need to implement the ApiModule interface . API-generated html documentation The BioLockJ documentation is stored in markdown files and rendered into html using mkdocs. The BioLockJ API is designed to generate a markdown document, which is ready to be rendered into an html file using mkdocs. Built-in descriptions Override the getCitationString() method. This should include citation information for any tool that your module wraps and a credit to yourself for creating the wrapper. Override the getDescription() method to return a short description of what your module does, this should be one to two sentences. For a more extensive description, including details about properties, expected inputs, assumptions, etc; override the getDetails() method (optional). Documenting Properties If your module introduces any NEW configuration properties, those properties should registered to the module so the API can retrieve them. Register properties using the addNewProperty() method in the modules constructor. For example, the GenMod module defines three properties: public GenMod() { super(); addNewProperty( PARAM, Properties.STRING_TYPE, \"parameters to pass to the user's script\" ); addNewProperty( SCRIPT, Properties.FILE_PATH, \"path to user script\" ); addNewProperty( LAUNCHER, Properties.STRING_TYPE, LAUNCHER_DESC ); } protected static final String PARAM = \"genMod.param\"; protected static final String SCRIPT = \"genMod.scriptPath\"; /** * {@link biolockj.Config} property: {@value #LAUNCHER}<br> * {@value #LAUNCHER_DESC} */ protected static final String LAUNCHER = \"genMod.launcher\"; private static final String LAUNCHER_DESC = \"Define executable language command if it is not included in your $PATH\"; In this example, the descriptions for PARAM and SCRIPT are written in the addNewProperty() method. The description for LAUNCHER is stored as its own string ( LAUNCHER_DESC ), and that string is referenced in the addNewProperty method and in the javadoc description for LAUNCHER . This rather verbose option IS NOT necissary, but it allows the description to be viewed through the api AND through javadocs, and IDE's; this is appropriate if you expect other classes to use the properties defined in your module. The descriptions for properties should be brief. Additional details such as interactions between properties or the effects of different values should be part of the getDetails() method. It should always be clear to a user what will happen if the value is \"null\". If there is a logical default for the property, that can passed as an additional argument to addNewProperty() . This value will only be used if there is no value given for the property in the config file (including any defaultProps layers and standard.properties). If your module uses any general properties (beyond any uses by the the super class), then you should register it in the module's constructor using the addGeneralProperty() method. public QiimeClosedRefClassifier() { super(); addGeneralProperty( Constants.EXE_AWK ); } The existing description and type for this property (defined in biolockj.Properties) will be returned if the module is queried about this property. For a list of general properties, run: biolockj_api listProps Finally, to very polished, you should override the isValidProp() method. Be sure to include the call to super. @Override public Boolean isValidProp( String property ) throws Exception { Boolean isValid = super.isValidProp( property ); switch(property) { case HN2_KEEP_UNINTEGRATED: try {Config.getBoolean( this, HN2_KEEP_UNINTEGRATED );} catch(Exception e) { isValid = false; } isValid = true; break; case HN2_KEEP_UNMAPPED: try {Config.getBoolean( this, HN2_KEEP_UNMAPPED );} catch(Exception e) { isValid = false; } isValid = true; break; } return isValid; } In the example above, the Humann2Parser module uses two properties that are not used by any super class. The call to super.isValidProp( property ) tests the property if it is used by a super class. This class only adds checks for its newly defined properties. Any property that is not tested, but is registered in the modules constructor will return true. This method is called through the API, and should be used to test one property at a time as if that is the only property in the config file. Tests to make sure that multiple properties are compatiable with each other should go in the checkDependencies() method. Using External Modules To use a module that you have created yourself or aquired from a third party, you need to: Save the compiled code in a folder on your machine, for example: /Users/joe/biolockjModules/JoesMods.jar Include your module in the module run order in your config file, for example: #BioModule com.joesCode.biolockj.RunTool Be sure to include any properties your module needs in the config file. Use the --external-modules <dir> option when you call biolockj: biolockj --external-modules /Users/joe/biolockjModules myPipeline.properties Any other modules you have made or aquired can also be in the /Users/joe/biolockjModules folder. Finding and Sharing Modules The official repository for external BioLockJ modules is blj_ext_modules . Each module has a folder at the top level of the repository and should include the java code as well a config file to test the module alone, a test file to run a multi-module pipeline that includes the module, and (where applicable) a dockerfile. This is work in progress.","title":"Building Modules"},{"location":"Building-Modules/#building-new-modules","text":"Any Java class that implements the BioModule interface can be added to a BioLockJ pipeline. The BioLockJ v1.0 implementation is currently focused on metagenomics analysis, but the generalized application framework is not limited to this domain. Users can implement new BioModules to automate a wide variety of bioinformatics and report analytics. The BioModule interface was designed so that users can develop new modules on their own.","title":"Building New Modules"},{"location":"Building-Modules/#beginners","text":"See the BioModule hello world tutorial.","title":"Beginners"},{"location":"Building-Modules/#coding-your-module","text":"To create a new BioModule , simply extend one of the abstract Java superclasses, code it's abstract methods, and add it to your pipeline with #BioModule tag your Config file:","title":"Coding your module"},{"location":"Building-Modules/#to-support-a-new-classifier-create-3-modules-that-implement-the-following-interfaces","text":"ClassifierModule : Implement to generate bash scripts needed to call classifier program ParserModule : Implement to parse classifier output, configure as classifier post-requisite OtuNode : Classifier specific implementation holds OTU information for 1 sequence","title":"To support a new classifier, create 3 modules that implement the following interfaces:"},{"location":"Building-Modules/#biomoduleimpl-is-the-top-level-superclass-for-all-modules","text":"Method Description checkDependencies() Must override. Called before executeTask() to identify Configuration errors and perform runtime validations. executeTask() Must override. Executes core module logic. cleanUp() Called after executeTask() to run cleanup operations, update Config properties, etc. getInputFiles() Return previous module output. getModuleDir() Return module root directory. getOutputDir() Return module output directory. getPostRequisiteModules() Returns a list of BioModules to run after the current module. getPreRequisiteModules() Returns a list of BioModules to run before the current module. getSummary() Return output directory summary. Most modules override this method by adding module specific summary details to super.getSummary(). getTempDir() Return module temp directory. setModuleDir(path) Set module directory.","title":"BioModuleImpl is the top-level superclass for all modules."},{"location":"Building-Modules/#scriptmoduleimpl-extends-biomoduleimpl-superclass-for-script-generating-modules","text":"Method Description buildScript(files) Must override. Called by executeTask() for datasets with forward reads only. The return type is a list of lists. Each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from getInputFiles(). buildScriptForPairedReads(files) Calls back to buildScript(files) by default. Subclasses override this method to generate unique scripts for datasets containing paired reads. checkDependencies() Called before executeTask() to validate script.batchSize , script.exitOnError , script.numThreads , script.permissions , script.timeout getJobParams() Return shell command to execute the MAIN script. getScriptDir() Return module script directory. getSummary() Adds the script directory summary to super.getSummary(). Most modules override this method by adding module specific summary details to super.getSummary(). getTimeout() Return script.timeout . getWorkerScriptFunctions() Return bash script lines for any functions needed in the worker scripts.","title":"ScriptModuleImpl extends BioModuleImpl:  superclass for script-generating modules."},{"location":"Building-Modules/#javamoduleimpl-extends-scriptmoduleimpl-superclass-for-pure-java-modules","text":"To avoid running code on the cluster head node, a temporary instance of BioLockJ is spawned on a cluster node which is launched by the sole worker script from the job queue. Method Description runModule() Must override. Executes core module logic. buildScript(files) This method returns a single line calling java on the BioLockJ source code, passing -d parameter to run in direct mode and the full class name of the JavaModule to indicate the module to run. getSource() Determines if running code from Jar or source code in order to write valid bash script lines. getTimeout() Return java.timeout . moduleComplete() Create the script success indicator file. moduleFailed() Create the script failures indicator file.","title":"JavaModuleImpl extends ScriptModuleImpl: superclass for pure Java modules."},{"location":"Building-Modules/#classifiermoduleimpl-extends-scriptmoduleimpl-biolockjmoduleclassifier-superclass","text":"Method Description buildScriptForPairedReads(files) Called by executeTask() for datasets with paired reads. The return type is a list of lists, where each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from SeqUtil .getPairedReads(getInputFiles()). checkDependencies() Validate Configuration properties exe.classifier and exe.classifierParams , verify sequence file format, log classifier version info, and verify no biolockj.module.seq modules are configured run after the ClassifierModule . Subclasses should call super.checkDependencies() if overriding this method to retain these verifications. executeTask() Call buildScript(files) or buildScriptForPairedReads(files) based input sequence format and calls BashScriptBuilder to generate the main script + 1 worker script for every script.batchSize samples. To change the batch scheme, override this method to call the alternate BashScriptBuilder .buildScripts() method signiture and hard code the batch size. All biolockj.module.classifier modules override this method. getClassifierExe() Return Configuration property exe.classifier to call the classifier program in the bash scripts. If the classifier is not included in cluster.modules , validate that value is a valid file path. If exe.classifier is undefined, replace the property prefix exe with the lowercase prefix of the module class name (less the standard module suffix classifier ). For example, use rdp.classifier for RdpClassifier and kraken.classifier for KrakenClassifier . This allows users to define all classifier programs in a default Configuration file rather than setting exe.clssifier in each project Configuration file. getClassifierParams() Return Configuration property exe.classifierParams which may contain a list of parameters (without hyphens) to pass to the classifier program in the bash scripts. If exe.classifierParams is undefined, replace the property prefix exe with the lowercase prefix of the module class name as described for exe.classifier . getSummary() Adds input directory summary to super.getSummary(). Most modules override this method to add module specific summary details to super.getSummary(). logVersion() Run exe.classifier --version to log version info. RDP overrides this method to return null since the version switch is not supported.","title":"ClassifierModuleImpl extends ScriptModuleImpl: biolockj.module.classifier superclass."},{"location":"Building-Modules/#parsermoduleimpl-extends-javamoduleimpl-biolockjmoduleimplicitparser-superclass","text":"Method Description parseSamples() Must override. Called by executeTask() to populate the Set returned by getParsedSamples(). Each classifier requires a unique parser module to decode its output. This method should iterate through the classifier reports to build OtuNode s for each sample-OTU found in the report. The OtuNode s are stored in a ParsedSample and cached via addParsedSample( ParsedSample ). addParsedSample( sample ) Add the ParsedSample to the Set returned by getParsedSamples(). buildOtuTables() Generate OTU abundance tables from ClassifierModule output. checkDependencies() Validate Configuration properties ( report.minOtuCount , report.minOtuThreshold , report.logBase ) and verify no biolockj.module.classifier modules are configured to run after the ParserModule . executeTask() If report.numHits =Y, add \"Num_Hits\" column to metadata containing the number of reads that map to any OTU for each sample. Calls buildOtuTables() to generate module output. getParsedSample(id) Return the ParsedSample from the the Set returned by getParsedSamples() for a given id. getParsedSamples() Return 1 ParsedSample for each classified sample in the dataset.","title":"ParserModuleImpl extends JavaModuleImpl: biolockj.module.implicit.parser superclass."},{"location":"Building-Modules/#otunodeimpl-is-the-superclass-for-the-biolockjnode-package","text":"Method Description addOtu(level, otu) A node represents a single OTU, each level in the taxonomic hierarchy is populated with this method. getCount() Get the OTU count. getLine() Get the classifier report line used to create the node. getOtuMap() This map may contain 1 element for each of the report.taxonomyLevels and is populated by addOtu(level, otu). getSampleId() Get the sample ID to which the OTU belongs. report() Print node info to log file as DEBUG line - not visible unless pipeline.logLevel=DEBUG . setCount(num) Set the OTU count. setLine(line) Set the classifier report line used to create the node. setSampleId(id) set the sample ID to which the OTU belongs. OtuNodeImpl methods do not need to be overridden. New OtuNode implementations should call existing methods from their constructor.","title":"OtuNodeImpl is the superclass for the biolockj.node package."},{"location":"Building-Modules/#document-your-module","text":"The BioLockJ API allows outside resources to get information about the BioLockJ program and any available modules. To interface with the API, your module will need to implement the ApiModule interface .","title":"Document your module"},{"location":"Building-Modules/#api-generated-html-documentation","text":"The BioLockJ documentation is stored in markdown files and rendered into html using mkdocs. The BioLockJ API is designed to generate a markdown document, which is ready to be rendered into an html file using mkdocs.","title":"API-generated html documentation"},{"location":"Building-Modules/#built-in-descriptions","text":"Override the getCitationString() method. This should include citation information for any tool that your module wraps and a credit to yourself for creating the wrapper. Override the getDescription() method to return a short description of what your module does, this should be one to two sentences. For a more extensive description, including details about properties, expected inputs, assumptions, etc; override the getDetails() method (optional).","title":"Built-in descriptions"},{"location":"Building-Modules/#documenting-properties","text":"If your module introduces any NEW configuration properties, those properties should registered to the module so the API can retrieve them. Register properties using the addNewProperty() method in the modules constructor. For example, the GenMod module defines three properties: public GenMod() { super(); addNewProperty( PARAM, Properties.STRING_TYPE, \"parameters to pass to the user's script\" ); addNewProperty( SCRIPT, Properties.FILE_PATH, \"path to user script\" ); addNewProperty( LAUNCHER, Properties.STRING_TYPE, LAUNCHER_DESC ); } protected static final String PARAM = \"genMod.param\"; protected static final String SCRIPT = \"genMod.scriptPath\"; /** * {@link biolockj.Config} property: {@value #LAUNCHER}<br> * {@value #LAUNCHER_DESC} */ protected static final String LAUNCHER = \"genMod.launcher\"; private static final String LAUNCHER_DESC = \"Define executable language command if it is not included in your $PATH\"; In this example, the descriptions for PARAM and SCRIPT are written in the addNewProperty() method. The description for LAUNCHER is stored as its own string ( LAUNCHER_DESC ), and that string is referenced in the addNewProperty method and in the javadoc description for LAUNCHER . This rather verbose option IS NOT necissary, but it allows the description to be viewed through the api AND through javadocs, and IDE's; this is appropriate if you expect other classes to use the properties defined in your module. The descriptions for properties should be brief. Additional details such as interactions between properties or the effects of different values should be part of the getDetails() method. It should always be clear to a user what will happen if the value is \"null\". If there is a logical default for the property, that can passed as an additional argument to addNewProperty() . This value will only be used if there is no value given for the property in the config file (including any defaultProps layers and standard.properties). If your module uses any general properties (beyond any uses by the the super class), then you should register it in the module's constructor using the addGeneralProperty() method. public QiimeClosedRefClassifier() { super(); addGeneralProperty( Constants.EXE_AWK ); } The existing description and type for this property (defined in biolockj.Properties) will be returned if the module is queried about this property. For a list of general properties, run: biolockj_api listProps Finally, to very polished, you should override the isValidProp() method. Be sure to include the call to super. @Override public Boolean isValidProp( String property ) throws Exception { Boolean isValid = super.isValidProp( property ); switch(property) { case HN2_KEEP_UNINTEGRATED: try {Config.getBoolean( this, HN2_KEEP_UNINTEGRATED );} catch(Exception e) { isValid = false; } isValid = true; break; case HN2_KEEP_UNMAPPED: try {Config.getBoolean( this, HN2_KEEP_UNMAPPED );} catch(Exception e) { isValid = false; } isValid = true; break; } return isValid; } In the example above, the Humann2Parser module uses two properties that are not used by any super class. The call to super.isValidProp( property ) tests the property if it is used by a super class. This class only adds checks for its newly defined properties. Any property that is not tested, but is registered in the modules constructor will return true. This method is called through the API, and should be used to test one property at a time as if that is the only property in the config file. Tests to make sure that multiple properties are compatiable with each other should go in the checkDependencies() method.","title":"Documenting Properties"},{"location":"Building-Modules/#using-external-modules","text":"To use a module that you have created yourself or aquired from a third party, you need to: Save the compiled code in a folder on your machine, for example: /Users/joe/biolockjModules/JoesMods.jar Include your module in the module run order in your config file, for example: #BioModule com.joesCode.biolockj.RunTool Be sure to include any properties your module needs in the config file. Use the --external-modules <dir> option when you call biolockj: biolockj --external-modules /Users/joe/biolockjModules myPipeline.properties Any other modules you have made or aquired can also be in the /Users/joe/biolockjModules folder.","title":"Using External Modules"},{"location":"Building-Modules/#finding-and-sharing-modules","text":"The official repository for external BioLockJ modules is blj_ext_modules . Each module has a folder at the top level of the repository and should include the java code as well a config file to test the module alone, a test file to run a multi-module pipeline that includes the module, and (where applicable) a dockerfile. This is work in progress.","title":"Finding and Sharing Modules"},{"location":"Built-in-modules/","text":"BioModules Some modules are packaged with BioLockJ (see below). To use modules created by a third-party, add the compiled files (jar file) to your biolockj extentions folder. When you call biolockj , use the --external-modules arg to pass in the location of the extra modules: biolockj --external-modules </path/to/extentions/folder> <config.properties> To create your own modules, see Building-Modules . In all cases, add modules to your BioModule order section to include them in your pipeline. Built-in BioModules: classifiers r16s classifiers wgs classifiers implicit modules implicit parsers module.implicit.parser.r16s.md module.implicit.parser.wgs.md implicit qiime modules report modules humann2 report by otu report by taxon R reports sequence modules DIY modules GenMod","title":"Modules"},{"location":"Built-in-modules/#biomodules","text":"Some modules are packaged with BioLockJ (see below). To use modules created by a third-party, add the compiled files (jar file) to your biolockj extentions folder. When you call biolockj , use the --external-modules arg to pass in the location of the extra modules: biolockj --external-modules </path/to/extentions/folder> <config.properties> To create your own modules, see Building-Modules . In all cases, add modules to your BioModule order section to include them in your pipeline.","title":"BioModules"},{"location":"Built-in-modules/#built-in-biomodules","text":"","title":"Built-in BioModules:"},{"location":"Built-in-modules/#classifiers","text":"r16s classifiers wgs classifiers","title":"classifiers"},{"location":"Built-in-modules/#implicit-modules","text":"implicit parsers module.implicit.parser.r16s.md module.implicit.parser.wgs.md implicit qiime modules","title":"implicit modules"},{"location":"Built-in-modules/#report-modules","text":"humann2 report by otu report by taxon R reports","title":"report modules"},{"location":"Built-in-modules/#sequence-modules","text":"","title":"sequence modules"},{"location":"Built-in-modules/#diy-modules","text":"GenMod","title":"DIY modules"},{"location":"Check-Dependencies/","text":"BioLockJ is designed find all problems in one sitting. Every module includes a check dependencies method, which quickly detects issues that would cause an error during execution. This is run for all modules in a pipeline before the first module executes. When BioLockJ runs, it has three major phases: pipeline formation - string together the modues specified in the config file along with any additional modules that the program adds on the users behalf; and initiate the utilities needed for the pipeline (such as docker, metadata, determine input type). check dependencies - scan the pipeline for anything that may cause an error during execution run pipeline - execute each module in the sequence. Precheck a pipeline By including the --precheck-only argument (or -p ) when running biolockj ; you are running in precheck mode. BioLockJ will do the first two phases, and then stop. This allows you to quickly test changes to your pipeline configuration without actually running a pipeline. It also allows you to see any modules that are automatically added to your pipeline.","title":"Check Dependencies"},{"location":"Check-Dependencies/#precheck-a-pipeline","text":"By including the --precheck-only argument (or -p ) when running biolockj ; you are running in precheck mode. BioLockJ will do the first two phases, and then stop. This allows you to quickly test changes to your pipeline configuration without actually running a pipeline. It also allows you to see any modules that are automatically added to your pipeline.","title":"Precheck a pipeline"},{"location":"Commands/","text":"The BioLockJ program is launched through the biolockj script. See biolockj --help . Support programs can access information about BioLockJ modules and properties through biolockj-api . There are also several helper scripts for small specific tasks, these are all found under $BLJ/script and added to the $PATH after the basic installation: Command Description blj_go Go to most recent $BLJ_PROJ pipeline & list contents. blj_log Tail last 1K lines from current or most recent $BLJ_PROJ pipeline log file. blj_summary Print current or most recent $BLJ_PROJ pipeline summary. blj_complete Manually completes the current module and pipeline status. blj_reset Reset pipeline status to incomplete. If restarted, execution will start with the current module. blj_downlaod If on cluster, print command syntax to download current or most recent $BLJ_PROJ pipeline analysis to your local workstation directory: pipeline.downloadDir .","title":"Commands"},{"location":"Configuration/","text":"Configuration files contain all system properties, program inputs, cutoff values, external dependencies, and format specifications used during pipeline execution. BioLockJ takes a single configuration file as a runtime parameter. Although all properties can be configured in one file, we recommend chaining default files through the pipeline.defaultProps option. This can often improve the portability, maintainability, and readability of the project-specific configuration files. Standard Properties BioLockJ will always apply the standard.properties file packaged with BioLockJ under resources/config/default/ ; you do not need to specify this file in your pipeline.defaultProps chain. IFF running a pipeline in docker, then BioLockJ will apply the docker.properties file packaged with BioLockJ under resources/config/default/ . User-specified Defaults We recommend creating an environment.properties file to assign envionment-specific defaults. Set cluster & script properties Set paths to key executables through exe properties Override standard.properties as needed. This information is the same for many (or all) projects run in this environment, and entering the info anew for each project is tedious, time-consuming and error-prone. If using a shared system, consider using a user.properties file. Set user-specific properties such as download.dir and mail.to. For shared projects, use a path that will be updated per-user, such as ~/biolock_user.properties Other logical intermediates my also present themselves. For example, some group of projects may need to override several of the defaults set in environmment.properties, but others still use the those defaults. Projects in this set can use pipeline.defaultProps=group2.properties and the group2.properties files may include pipeline.defaultProps=environment.properties Project Properties Create a new configuration file for each pipeline to assign project-specific properties: Set the BioModule execution order Set pipeline.defaultProps = environment.properties You may use multiple default config files: pipeline.defaultProps=environment.properties,groupSettings.properties Override environment.properties and standard.properties as needed Example project configuration files can be found in templates . If the same property is given in multiple config files, the highest priority goes to the file used to launch the pipeline. Standard.properties always has the lowest priority. A copy of each configuration file is stored in the pipeline root directory to serve as primary project documentation. BioModule execution order To include a BioModule in your pipeline, add a #BioModule line to the top your configuration file, as shown in the examples found in templates . Each line has the #BioModule keyword followed by the path for that module. For example: #BioModule biolockj.module.seq.PearMergeReads #BioModule biolockj.module.classifier.wgs.Kraken2Classifier #BioModule biolockj.module.report.r.R_PlotMds BioModules will be executed in the order they are listed in here. A typical pipeline contians one classifier module . Any number of sequence pre-processing modules may come before the classifier module. Any number of report modules may come after the classifier module. In addition to the BioModules specified in the configuration file, BioLockJ may add implicit modules that the are required by specified modules. See Example Pipeline . A module can be given an alias by using the AS keyword in its execution line: #BioModule biolockj.module.seq.PearMergeReads AS Pear This is is generally used for modules that are used more than once in the same pipeline. Given this alias, the folder for this module will be called 01_Pear instead of 01_PearMergeReads , and any general properties directed to this module would use the prefix Pear instead of PearMergedReads . An alias must start with a capital letter, and cannot duplicate a name/alias of any other module in the same pipeline. Summary of Properties Properties are defined as name-value pairs. List-values are comma separated. Leading and trailing whitespace is removed so \"propName=x,y\" is equivalent to \"propName = x, y\". Some pipeline properties (usually those used by pipeline utilities) can be directed to a specific module. For example, script.numThreads is a general property that specifies that number of threads alloted to each script launched by any module; and PearMergeReads.numThreads overrides that property ONLY for the PearMergeReads module. pipeline.defaultProps is a handled before any other property. It is used to link another properties file. The properties from that file are added to the MASTER set. The defaultProps property is not included in the MASTER properties set. exe. properties are used to specify the path to common executables. Modules are sometimes written to use a common tool, such as Rscript or bowtie . These modules will write scripts with the assumption that this command is on the $PATH when the script is executed UNLESS exe.Rscript is given specifying a path to use. The exe. properties are often specified in a defaultProps file for a given environment rather than in individual project properties files. If you are running a pipeline using docker, it is assumed that all file paths in your config file are written in terms of your host machine. The EXCEPTION to this is the exe. file paths. Most often, docker containers are used because of the executables baked into them. In the rare case where you want to use an executable from your local machine, while running a pipeline in docker, you can specify this by using the prefix hostExe. in place of exe. . aws Property Description aws.profile String aws.ram AWS memory applied through Nextflow. example value: \"8 GB\" aws.stack String aws.s3 String cluster Property Description cluster.batchCommand The command to submit jobs on the cluster cluster.host Cluster host address cluster.jobHeader Job script header to define # of nodes, # of cores, RAM, walltime, etc. cluster.modules List of modules to load before execution. Adds \u201cmodule load\u201d command to bash scripts cluster.prologue Command(s) to run at the start of every script after loading cluster modules (if any) cluster.runJavaAsScript Options: Y/N. If Y, each JavaModule will instantiate a clone of the application in direct mode on a job node via a single worker script to avoid overworking the head node where BioLockJ is deployed cluster.validateParams Options: Y/N. If Y, validate cluster.jobHeader \"ppn:\" or \"procs:\" value matches script.numThreads demultiplexer Property Description demultiplexer.barcodeCutoff desc demultimplexer.barcodeRevComp Options: Y/N. Use reverse compliment of metadata.barcodeColumn if demultimplexer.strategy = barcode_in_header or barcode_in_seq. demultimplexer.strategy Options: barcode_in_header, barcode_in_seq, id_in_header, do_not_demux. Set the Demultiplexer strategy. If using barcodes, they must be provided in the metadata.filePath with in column name defined by metadata.barcodeColumn . docker Property Description docker.imgVersion By default, docker will always use 'latest', but advanced users may specify a different tag. docker.user Docker Hub user name with the BioLockJ containers. By default the \"biolockj\" user is used to pull the standard modules, but advanced users can deploy their own versions of these modules and add new modules in their own Docker Hub account. docker.saveContainerOnExit Y/N. If Y, property removed the default --rm flag on docker run command exe Property Description exe.awk Define executable awk command, if default \"awk\" is not included in your $PATH exe.docker Define executable docker command, if default \"docker\" is not included in your $PATH exe.gzip Define executable gzip command, if default \"gzip\" is not included in your $PATH exe.humann2 Define executable humann2 command, if default \"humann2\" is not included in your $PATH exe.humann2Params Optional humann2 parameters exe.humann2JoinTableParams Optional parameters exe.humann2RenormTableParams Optional parameters exe.java Define executable java command, if default \"java\" is not included in your $PATH exe.javaParams Optional parameters exe.kneaddata Define executable kneaddata command, if default \"kneaddata\" is not included in your $PATH exe.kneaddataParams Optional kneaddata parameters exe.kraken Define executable kraken command, if default \"kraken\" is not included in your $PATH exe.krakenParams Optional kraken parameters exe.kraken2 Define executable kraken2 command, if default \"kraken2\" is not included in your $PATH exe.kraken2Params Optional kraken2 parameters exe.metaphlan2 Define executable metaphlan2 command, if default \"metaphlan2\" is not included in your $PATH exe.metaphlan2Params Optional metaphlan2 parameters exe.pear Define executable pear command, if default \"pear\" is not included in your $PATH exe.pearParams Optional pear parameters exe.python Define executable python command, if default \"python\" is not included in your $PATH exe.Rscript Define executable Rscript command, if default \"Rscript\" is not included in your $PATH exe.vsearch Define executable vsearch command, if default \"vsearch\" is not included in your $PATH exe.vsearchParams Optional vsearch parameters GenMod Property Description genMod.launcher Define executable language command if it is not included in your $PATH genMod.param Any parameters that is needed for user's script genMod.scriptPath Path where user script is stored humann2 Property Description humann2.disableGeneFamilies Options: Y/N. If Y, disable HumanN2 Gene Family report humann2.disablePathAbundance Options: Y/N. If Y, disable HumanN2 Pathway Abundance report humann2.disablePathCoverage Options: Y/N. If Y, disable HumanN2 Pathway Coverage report humann2.keepUnintegrated Options: Y/N. If Y, keep UNINTEGRATED column in count tables (otherwise this column is dropped) humann2.keepUnmapped Options: Y/N. If Y, keep UNMAPPED column in count tables (otherwise this column is dropped) humann2.nuclDB Directory property may contain multiple nucleotide database files humann2.protDB Directory property may contain protein nucleotide database files input Property Description input.dirPaths List of directories containing pipeline input files input.ignoreFiles List of files to ignore if found in * input.dirPaths* input.requireCompletePairs Options: Y/N. Stop pipeline if any unpaired FW or RV read sequence file is found input.suffixFw File name suffix to indicate a forward read input.suffixRv File name suffix to indicate a reverse read input.trimPrefix For files named by Sample ID, provide the prefix preceding the ID to trim when extracting Sample ID. For multiplexed sequences, provide any characters in the sequence header preceding the ID. For fastq, this value could be \u201c@\u201d if the sample ID was added to the header immediately after the \"@\" symbol. input.trimSuffix For files named by Sample ID, provide the suffix after the ID, often this is just the file extension. Do not include read direction indicators listed in input.suffixFw/input.suffixRv . For multiplexed sequences, provide 1st character in the sequence header found after every embedded Sample ID. If undefined, \u201c_\u201d is used as the default end-of-sample-ID delimiter. kneaddata Property Description kneaddata.dbs Path to database for KneadData program kraken Property Description kraken.db Path to kraken database kraken2 Property Description kraken2.db Path to kraken2 database mail Property Description mail.encryptedPassword Encrypted password from email.from account. If BioLockJ is passed a 2nd parameter (in addition to the config file), the 2nd parameter should be the clear-text password. The password will be encrypted and stored in the prop file for future use. WARNING: Base64 encryption is only a trivial roadblock for malicious users. This functionality is intended merely to keep clear-text passwords out of the configuration files and should only be used with a disposable email.from account. mail.from Notification emails sent from this account, provided email.encryptedPassword is valid mail.smtp.auth Options: Y/N. Set the SMTP authorization property mail.smtp.host Email SMTP Host mail.smtp.port Email SMTP Host mail.smtp.starttls.enable Options: Y/N. Set the SMTP start TLS property mail.to Comma-separated email recipients list metadata Property Description metadata.barcodeColumn Metadata column name containing the barcode used for demultiplexing metadata.columnDelim Define column delimiter for metadata.filePath file, default = tab metadata.commentChar Define how comments are indicated in metadata.filePath file, default = \"\" metadata.fileNameColumn Column in metadata file giving file names used to identify each sample. Standard default: \"InputFileName\". Values should be simple names, not file paths, and unique to each sample. Using this column in the metadata overrides the use of input.trimPreifx and input.trimSuffix. For paired reads, give the forward read file and use input.suffixFw and input.suffixRv to link to the reverse file. metadata.filePath Metadata file path, must have unique column headers metadata.nullValue Define how null values are represented in metadata metadata.required Options: Y/N. Require every sequence file has a corresponding row in metadata file metadata.useEveryRow Options: Y/N. Requires every metadata row to have a corresponding sequence file metaphlan2 Property Description metaphlan2.db Directory property containing alternate database. Must always be paired with metaphlan2.mpa_pkl metaphlan2.mpa_pkl File property containing path to the mpa_pkl file used to reference an alternate DB. Must always be paired with metaphlan2.db multiplexer Property Description multiplexer.gzip Options: Y/N. If Y, gzip the multiplexed output pipeline Property Description pipeline.copyInput Options: Y/N. If Y, copy input.dirPaths into a new directory under the project root directory pipeline.defaultDemultiplexer Assign module to demultiplex datasets. Default = Demultiplexer pipeline.defaultFastaConverter Assign module to convert fastq sequence files into fasta format when required. Default = AwkFastaConverter pipeline.defaultSeqMerger Assign module to merge paired reads when required. Default = PearMergeReads pipeline.defaultStatsModule Java class name for default module used generate p-value and other stats pipeline.defaultProps Path to a default BioLockJ configuration file containing default property values that are overridden if defined in the primary configuration file pipeline.deleteTempFiles Options: Y/N. If Y, delete module temp dirs after execution pipeline.disableAddImplicitModules Options: Y/N. If Y, implicit modules will not be added to the pipeline pipeline.disableAddPreReqModules Options: Y/N. If Y, prerequisite modules will not be added to the pipeline. pipeline.downloadDir The pipeline summary includes an scp command for the user to download the pipeline analysis if executed on a cluster server. This property defines the target directory on the users workstation to which the analysis will be downloaded. pipeline.env Options: aws, cluster, local. Describes runtime environment pipeline.limitDebugClasses used to limit classes that log debug statements pipeline.logLevel Options: DEBUG, INFO, WARN, ERROR. Determines Java log level sensitivity pipeline.permissions Set chmod -R command security bits on pipeline root directory (Ex. 770) pipeline.userProfile Bash users typically use ~/.bash_profile (the standard default). qiime Property Description qiime.alphaMetrics Options listed online: scikit-bio.org qiime.params Optional parameters passed to qiime scripts qiime.pynastAlignDB File property to define ~/.qiime_config pynast_template_alignment_fp. If supplied, qiime.refSeqDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.refSeqDB File property to define ~/.qiime_config pick_otus_reference_seqs_fp and assign_taxonomy_reference_seqs_fp. If supplied, qiime.pynastAlignDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.removeChimeras Options: Y/N. If Y, remove chimeras after open or de novo OTU picking using exe.vsearch qiime.taxaDB File property to define ~/.qiime_config assign_taxonomy_id_to_taxonomy_fp. If supplied, qiime.pynastAlignDB and qiime.refSeqDB must also be supplied and all three must share some parent directory. r Property Description r.colorBase This is the base color used for labels & headings in the PDF report r.colorHighlight This color is used to highlight significant OTU plot titles r.colorPalette palette argument passed to get_palette {ggpubr} to select colors for some output visualiztions r.colorPoint Sets the color of scatterplot and strip-chart plot points r.debug Options: Y/N. If Y, will generate R Script log files r.excludeFields List metadata columns to exclude from R script reports r.nominalFields Explicitly override default field type assignment to model as a nominal field in R r.numericFields Explicitly override default field type assignment to model as a numeric field in R r.pch Sets R plot pch parameter for PDF report r.pvalCutoff Sets p-value cutoff used to assign label r.colorHighlight r.pValFormat Sets the format used in R sprintf() function r.rareOtuThreshold If >=1, R will filter OTUs found in fewer than this many samples. If <1, R will interperate the value as a percentage and discard OTUs not found in at least that percentage of samples r.reportFields Override field used to explicitly list metadata columns to report in the R scripts. If left undefined, all columns are reported r.saveRData Options: Y/N. If Y, all R script generating BioModules will save R Session data to the module output directory to a file using the extension \".RData\" r.timeout Sets # minutes before R Script will time out and fail r_CalculateStats Property Description r_CalculateStats.pAdjustScope Options: GLOBAL, LOCAL, TAXA, ATTRIBUTE. Used to set the p.adjust \"n\" parameter for how many simultaneous p-value calculations r_CalculateStats.pAdjustMethod Sets the p.adjust \"method\" parameter r_PlotEffectSize Property Description r_PlotEffectSize.parametricPval Options: Y/N. If Y, the parametric p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the non-parametric p-value is used. r_PlotEffectSize.disablePvalAdj Options: Y/N. If Y, the non-adjusted p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the adjusted p-value is used. r_PlotEffectSize.excludePvalAbove Options: [0,1], Taxa with a p-value above this value are excluded from the plot. r_PlotEffectSize.taxa Override other criteria for selecting which taxa to include in the plot by specifying wich taxa should be included r_PlotEffectSize.maxNumTaxa Each plot is given one page. This is the maximum number of bars to include in each one-page plot. r_PlotEffectSize.disableCohensD Options: Y/N. If N (default), produce plots for binary attributes showing effect size calculated as Cohen's d. If Y, skip this plot type. r_PlotEffectSize.disableRSquared Options: Y/N. If N (default), produce plots showing effect size calculated as the r-squared value. If Y, skip this plot type. r_PlotEffectSize.disableFoldChange Options: Y/N. If N (default), produce plots for binary attributes showing the fold change. If Y, skip this plot type. r_PlotMds Property Description r_PlotMds.numAxis Sets # MDS axis to plot r_PlotMds.distance distance metric for calculating MDS (default: bray) r_PlotMds.reportFields Override field used to explicitly list metadata columns to build MDS plots. If left undefined, all columns are reported rarefyOtuCounts Property Description rarefyOtuCounts.iterations Positive integer. The number of iterations to randomly select the rarefyOtuCounts.quantile of OTUs rarefyOtuCounts.lowAbundantCutoff Minimum percentage of samples that must contain an OTU. rarefyOtuCounts.quantile Quantile for rarefication. The number of OTUs/sample are ordered, all samples with more OTUs than the quantile sample are subselected without replacement until they have the same number of OTUs as the quantile sample rarefyOtuCounts.rmLowSamples Options: Y/N. If Y, all samples below the rarefyOtuCounts.quantile quantile sample are removed rarefySeqs Property Description rarefySeqs.max Randomly select maximum number of sequences per sample rarefySeqs.min Discard samples without minimum number of sequences rdp Property Description rdp.db File property used to define an alternate RDP database file rdp.jar File property for RDP java executable JAR rdp.minThresholdScore Required RDP minimum threshold score for valid OTUs report Property Description report.logBase Options: 10/e. If e, use natural log (base e), otherwise use log base 10 report.minCount Integer, minimum table count allowed. If a count less that this value is found, it is set to 0. report.numHits Options: Y/N. If Y, and add Num_Hits to metadata report.numReads Options: Y/N. If Y, and add Num_Reads to metadata report.scarceCountCutoff Minimum percentage of samples that must contain a count value for it to be kept. report.scarceSampleCutoff Minimum percentage of data columns that must be non-zero to keep the sample. report.taxonomyLevels Options: domain, phylum, class, order, family, genus, species. Generate reports for listed taxonomy levels script Property Description script.batchSize Number of sequence files to process per worker script script.defaultHeader Used to set shebang line to define scripts as bash executables, such as \"#!/bin/bash\" script.numThreads Integer value passed to any module that takes a number of threads parameter script.permissions Set chmod command security bits on generated scripts (Ex. 770) script.timeout Integer, time (minutes) before worker scripts times out. seqFileValidator Property Description seqFileValidator.requireEqualNumPairs Options: Y/N. default Y. seqFileValidator.seqMaxLen maximum number of bases per read seqFileValidator.seqMinLen minimum number of bases per read trimPrimers Property Description trimPrimers.filePath Path to file containing one primer sequence per line. trimPrimers.requirePrimer Options: Y/N. If Y, TrimPrimers will discard reads that do not include a primer sequence. validation Property Description validation.compareOn Which columns in the expectation file should be used for the comparison. Options: name, size, md5. Default: use all columns in the expectation file. validation.disableValidation Turn off validation. No validation file output is produced. Options: Y/N. default: N validation.expectationFile File path to the table of expectations. If a directory is given, BioLockJ will look for a file named after the module being evaluated. validation.reportOn Which attributes of the file should be included in the validation report file. Options: name, size, md5 validation.sizeWithinPercent What percentage difference is permitted between an output file and its expectation. Options: any positive number validation.stopPipeline If enabled, the validation utlility will stop the pipeline if any module fails validation. Options: Y/N","title":"Configuration"},{"location":"Configuration/#standard-properties","text":"BioLockJ will always apply the standard.properties file packaged with BioLockJ under resources/config/default/ ; you do not need to specify this file in your pipeline.defaultProps chain. IFF running a pipeline in docker, then BioLockJ will apply the docker.properties file packaged with BioLockJ under resources/config/default/ .","title":"Standard Properties"},{"location":"Configuration/#user-specified-defaults","text":"We recommend creating an environment.properties file to assign envionment-specific defaults. Set cluster & script properties Set paths to key executables through exe properties Override standard.properties as needed. This information is the same for many (or all) projects run in this environment, and entering the info anew for each project is tedious, time-consuming and error-prone. If using a shared system, consider using a user.properties file. Set user-specific properties such as download.dir and mail.to. For shared projects, use a path that will be updated per-user, such as ~/biolock_user.properties Other logical intermediates my also present themselves. For example, some group of projects may need to override several of the defaults set in environmment.properties, but others still use the those defaults. Projects in this set can use pipeline.defaultProps=group2.properties and the group2.properties files may include pipeline.defaultProps=environment.properties","title":"User-specified Defaults"},{"location":"Configuration/#project-properties","text":"Create a new configuration file for each pipeline to assign project-specific properties: Set the BioModule execution order Set pipeline.defaultProps = environment.properties You may use multiple default config files: pipeline.defaultProps=environment.properties,groupSettings.properties Override environment.properties and standard.properties as needed Example project configuration files can be found in templates . If the same property is given in multiple config files, the highest priority goes to the file used to launch the pipeline. Standard.properties always has the lowest priority. A copy of each configuration file is stored in the pipeline root directory to serve as primary project documentation.","title":"Project Properties"},{"location":"Configuration/#biomodule-execution-order","text":"To include a BioModule in your pipeline, add a #BioModule line to the top your configuration file, as shown in the examples found in templates . Each line has the #BioModule keyword followed by the path for that module. For example: #BioModule biolockj.module.seq.PearMergeReads #BioModule biolockj.module.classifier.wgs.Kraken2Classifier #BioModule biolockj.module.report.r.R_PlotMds BioModules will be executed in the order they are listed in here. A typical pipeline contians one classifier module . Any number of sequence pre-processing modules may come before the classifier module. Any number of report modules may come after the classifier module. In addition to the BioModules specified in the configuration file, BioLockJ may add implicit modules that the are required by specified modules. See Example Pipeline . A module can be given an alias by using the AS keyword in its execution line: #BioModule biolockj.module.seq.PearMergeReads AS Pear This is is generally used for modules that are used more than once in the same pipeline. Given this alias, the folder for this module will be called 01_Pear instead of 01_PearMergeReads , and any general properties directed to this module would use the prefix Pear instead of PearMergedReads . An alias must start with a capital letter, and cannot duplicate a name/alias of any other module in the same pipeline.","title":"BioModule execution order"},{"location":"Configuration/#summary-of-properties","text":"Properties are defined as name-value pairs. List-values are comma separated. Leading and trailing whitespace is removed so \"propName=x,y\" is equivalent to \"propName = x, y\". Some pipeline properties (usually those used by pipeline utilities) can be directed to a specific module. For example, script.numThreads is a general property that specifies that number of threads alloted to each script launched by any module; and PearMergeReads.numThreads overrides that property ONLY for the PearMergeReads module. pipeline.defaultProps is a handled before any other property. It is used to link another properties file. The properties from that file are added to the MASTER set. The defaultProps property is not included in the MASTER properties set. exe. properties are used to specify the path to common executables. Modules are sometimes written to use a common tool, such as Rscript or bowtie . These modules will write scripts with the assumption that this command is on the $PATH when the script is executed UNLESS exe.Rscript is given specifying a path to use. The exe. properties are often specified in a defaultProps file for a given environment rather than in individual project properties files. If you are running a pipeline using docker, it is assumed that all file paths in your config file are written in terms of your host machine. The EXCEPTION to this is the exe. file paths. Most often, docker containers are used because of the executables baked into them. In the rare case where you want to use an executable from your local machine, while running a pipeline in docker, you can specify this by using the prefix hostExe. in place of exe. .","title":"Summary of Properties"},{"location":"Configuration/#aws","text":"Property Description aws.profile String aws.ram AWS memory applied through Nextflow. example value: \"8 GB\" aws.stack String aws.s3 String","title":"aws"},{"location":"Configuration/#cluster","text":"Property Description cluster.batchCommand The command to submit jobs on the cluster cluster.host Cluster host address cluster.jobHeader Job script header to define # of nodes, # of cores, RAM, walltime, etc. cluster.modules List of modules to load before execution. Adds \u201cmodule load\u201d command to bash scripts cluster.prologue Command(s) to run at the start of every script after loading cluster modules (if any) cluster.runJavaAsScript Options: Y/N. If Y, each JavaModule will instantiate a clone of the application in direct mode on a job node via a single worker script to avoid overworking the head node where BioLockJ is deployed cluster.validateParams Options: Y/N. If Y, validate cluster.jobHeader \"ppn:\" or \"procs:\" value matches script.numThreads","title":"cluster"},{"location":"Configuration/#demultiplexer","text":"Property Description demultiplexer.barcodeCutoff desc demultimplexer.barcodeRevComp Options: Y/N. Use reverse compliment of metadata.barcodeColumn if demultimplexer.strategy = barcode_in_header or barcode_in_seq. demultimplexer.strategy Options: barcode_in_header, barcode_in_seq, id_in_header, do_not_demux. Set the Demultiplexer strategy. If using barcodes, they must be provided in the metadata.filePath with in column name defined by metadata.barcodeColumn .","title":"demultiplexer"},{"location":"Configuration/#docker","text":"Property Description docker.imgVersion By default, docker will always use 'latest', but advanced users may specify a different tag. docker.user Docker Hub user name with the BioLockJ containers. By default the \"biolockj\" user is used to pull the standard modules, but advanced users can deploy their own versions of these modules and add new modules in their own Docker Hub account. docker.saveContainerOnExit Y/N. If Y, property removed the default --rm flag on docker run command","title":"docker"},{"location":"Configuration/#exe","text":"Property Description exe.awk Define executable awk command, if default \"awk\" is not included in your $PATH exe.docker Define executable docker command, if default \"docker\" is not included in your $PATH exe.gzip Define executable gzip command, if default \"gzip\" is not included in your $PATH exe.humann2 Define executable humann2 command, if default \"humann2\" is not included in your $PATH exe.humann2Params Optional humann2 parameters exe.humann2JoinTableParams Optional parameters exe.humann2RenormTableParams Optional parameters exe.java Define executable java command, if default \"java\" is not included in your $PATH exe.javaParams Optional parameters exe.kneaddata Define executable kneaddata command, if default \"kneaddata\" is not included in your $PATH exe.kneaddataParams Optional kneaddata parameters exe.kraken Define executable kraken command, if default \"kraken\" is not included in your $PATH exe.krakenParams Optional kraken parameters exe.kraken2 Define executable kraken2 command, if default \"kraken2\" is not included in your $PATH exe.kraken2Params Optional kraken2 parameters exe.metaphlan2 Define executable metaphlan2 command, if default \"metaphlan2\" is not included in your $PATH exe.metaphlan2Params Optional metaphlan2 parameters exe.pear Define executable pear command, if default \"pear\" is not included in your $PATH exe.pearParams Optional pear parameters exe.python Define executable python command, if default \"python\" is not included in your $PATH exe.Rscript Define executable Rscript command, if default \"Rscript\" is not included in your $PATH exe.vsearch Define executable vsearch command, if default \"vsearch\" is not included in your $PATH exe.vsearchParams Optional vsearch parameters","title":"exe"},{"location":"Configuration/#genmod","text":"Property Description genMod.launcher Define executable language command if it is not included in your $PATH genMod.param Any parameters that is needed for user's script genMod.scriptPath Path where user script is stored","title":"GenMod"},{"location":"Configuration/#humann2","text":"Property Description humann2.disableGeneFamilies Options: Y/N. If Y, disable HumanN2 Gene Family report humann2.disablePathAbundance Options: Y/N. If Y, disable HumanN2 Pathway Abundance report humann2.disablePathCoverage Options: Y/N. If Y, disable HumanN2 Pathway Coverage report humann2.keepUnintegrated Options: Y/N. If Y, keep UNINTEGRATED column in count tables (otherwise this column is dropped) humann2.keepUnmapped Options: Y/N. If Y, keep UNMAPPED column in count tables (otherwise this column is dropped) humann2.nuclDB Directory property may contain multiple nucleotide database files humann2.protDB Directory property may contain protein nucleotide database files","title":"humann2"},{"location":"Configuration/#input","text":"Property Description input.dirPaths List of directories containing pipeline input files input.ignoreFiles List of files to ignore if found in * input.dirPaths* input.requireCompletePairs Options: Y/N. Stop pipeline if any unpaired FW or RV read sequence file is found input.suffixFw File name suffix to indicate a forward read input.suffixRv File name suffix to indicate a reverse read input.trimPrefix For files named by Sample ID, provide the prefix preceding the ID to trim when extracting Sample ID. For multiplexed sequences, provide any characters in the sequence header preceding the ID. For fastq, this value could be \u201c@\u201d if the sample ID was added to the header immediately after the \"@\" symbol. input.trimSuffix For files named by Sample ID, provide the suffix after the ID, often this is just the file extension. Do not include read direction indicators listed in input.suffixFw/input.suffixRv . For multiplexed sequences, provide 1st character in the sequence header found after every embedded Sample ID. If undefined, \u201c_\u201d is used as the default end-of-sample-ID delimiter.","title":"input"},{"location":"Configuration/#kneaddata","text":"Property Description kneaddata.dbs Path to database for KneadData program","title":"kneaddata"},{"location":"Configuration/#kraken","text":"Property Description kraken.db Path to kraken database","title":"kraken"},{"location":"Configuration/#kraken2","text":"Property Description kraken2.db Path to kraken2 database","title":"kraken2"},{"location":"Configuration/#mail","text":"Property Description mail.encryptedPassword Encrypted password from email.from account. If BioLockJ is passed a 2nd parameter (in addition to the config file), the 2nd parameter should be the clear-text password. The password will be encrypted and stored in the prop file for future use. WARNING: Base64 encryption is only a trivial roadblock for malicious users. This functionality is intended merely to keep clear-text passwords out of the configuration files and should only be used with a disposable email.from account. mail.from Notification emails sent from this account, provided email.encryptedPassword is valid mail.smtp.auth Options: Y/N. Set the SMTP authorization property mail.smtp.host Email SMTP Host mail.smtp.port Email SMTP Host mail.smtp.starttls.enable Options: Y/N. Set the SMTP start TLS property mail.to Comma-separated email recipients list","title":"mail"},{"location":"Configuration/#metadata","text":"Property Description metadata.barcodeColumn Metadata column name containing the barcode used for demultiplexing metadata.columnDelim Define column delimiter for metadata.filePath file, default = tab metadata.commentChar Define how comments are indicated in metadata.filePath file, default = \"\" metadata.fileNameColumn Column in metadata file giving file names used to identify each sample. Standard default: \"InputFileName\". Values should be simple names, not file paths, and unique to each sample. Using this column in the metadata overrides the use of input.trimPreifx and input.trimSuffix. For paired reads, give the forward read file and use input.suffixFw and input.suffixRv to link to the reverse file. metadata.filePath Metadata file path, must have unique column headers metadata.nullValue Define how null values are represented in metadata metadata.required Options: Y/N. Require every sequence file has a corresponding row in metadata file metadata.useEveryRow Options: Y/N. Requires every metadata row to have a corresponding sequence file","title":"metadata"},{"location":"Configuration/#metaphlan2","text":"Property Description metaphlan2.db Directory property containing alternate database. Must always be paired with metaphlan2.mpa_pkl metaphlan2.mpa_pkl File property containing path to the mpa_pkl file used to reference an alternate DB. Must always be paired with metaphlan2.db","title":"metaphlan2"},{"location":"Configuration/#multiplexer","text":"Property Description multiplexer.gzip Options: Y/N. If Y, gzip the multiplexed output","title":"multiplexer"},{"location":"Configuration/#pipeline","text":"Property Description pipeline.copyInput Options: Y/N. If Y, copy input.dirPaths into a new directory under the project root directory pipeline.defaultDemultiplexer Assign module to demultiplex datasets. Default = Demultiplexer pipeline.defaultFastaConverter Assign module to convert fastq sequence files into fasta format when required. Default = AwkFastaConverter pipeline.defaultSeqMerger Assign module to merge paired reads when required. Default = PearMergeReads pipeline.defaultStatsModule Java class name for default module used generate p-value and other stats pipeline.defaultProps Path to a default BioLockJ configuration file containing default property values that are overridden if defined in the primary configuration file pipeline.deleteTempFiles Options: Y/N. If Y, delete module temp dirs after execution pipeline.disableAddImplicitModules Options: Y/N. If Y, implicit modules will not be added to the pipeline pipeline.disableAddPreReqModules Options: Y/N. If Y, prerequisite modules will not be added to the pipeline. pipeline.downloadDir The pipeline summary includes an scp command for the user to download the pipeline analysis if executed on a cluster server. This property defines the target directory on the users workstation to which the analysis will be downloaded. pipeline.env Options: aws, cluster, local. Describes runtime environment pipeline.limitDebugClasses used to limit classes that log debug statements pipeline.logLevel Options: DEBUG, INFO, WARN, ERROR. Determines Java log level sensitivity pipeline.permissions Set chmod -R command security bits on pipeline root directory (Ex. 770) pipeline.userProfile Bash users typically use ~/.bash_profile (the standard default).","title":"pipeline"},{"location":"Configuration/#qiime","text":"Property Description qiime.alphaMetrics Options listed online: scikit-bio.org qiime.params Optional parameters passed to qiime scripts qiime.pynastAlignDB File property to define ~/.qiime_config pynast_template_alignment_fp. If supplied, qiime.refSeqDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.refSeqDB File property to define ~/.qiime_config pick_otus_reference_seqs_fp and assign_taxonomy_reference_seqs_fp. If supplied, qiime.pynastAlignDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.removeChimeras Options: Y/N. If Y, remove chimeras after open or de novo OTU picking using exe.vsearch qiime.taxaDB File property to define ~/.qiime_config assign_taxonomy_id_to_taxonomy_fp. If supplied, qiime.pynastAlignDB and qiime.refSeqDB must also be supplied and all three must share some parent directory.","title":"qiime"},{"location":"Configuration/#r","text":"Property Description r.colorBase This is the base color used for labels & headings in the PDF report r.colorHighlight This color is used to highlight significant OTU plot titles r.colorPalette palette argument passed to get_palette {ggpubr} to select colors for some output visualiztions r.colorPoint Sets the color of scatterplot and strip-chart plot points r.debug Options: Y/N. If Y, will generate R Script log files r.excludeFields List metadata columns to exclude from R script reports r.nominalFields Explicitly override default field type assignment to model as a nominal field in R r.numericFields Explicitly override default field type assignment to model as a numeric field in R r.pch Sets R plot pch parameter for PDF report r.pvalCutoff Sets p-value cutoff used to assign label r.colorHighlight r.pValFormat Sets the format used in R sprintf() function r.rareOtuThreshold If >=1, R will filter OTUs found in fewer than this many samples. If <1, R will interperate the value as a percentage and discard OTUs not found in at least that percentage of samples r.reportFields Override field used to explicitly list metadata columns to report in the R scripts. If left undefined, all columns are reported r.saveRData Options: Y/N. If Y, all R script generating BioModules will save R Session data to the module output directory to a file using the extension \".RData\" r.timeout Sets # minutes before R Script will time out and fail","title":"r"},{"location":"Configuration/#r_calculatestats","text":"Property Description r_CalculateStats.pAdjustScope Options: GLOBAL, LOCAL, TAXA, ATTRIBUTE. Used to set the p.adjust \"n\" parameter for how many simultaneous p-value calculations r_CalculateStats.pAdjustMethod Sets the p.adjust \"method\" parameter","title":"r_CalculateStats"},{"location":"Configuration/#r_ploteffectsize","text":"Property Description r_PlotEffectSize.parametricPval Options: Y/N. If Y, the parametric p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the non-parametric p-value is used. r_PlotEffectSize.disablePvalAdj Options: Y/N. If Y, the non-adjusted p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the adjusted p-value is used. r_PlotEffectSize.excludePvalAbove Options: [0,1], Taxa with a p-value above this value are excluded from the plot. r_PlotEffectSize.taxa Override other criteria for selecting which taxa to include in the plot by specifying wich taxa should be included r_PlotEffectSize.maxNumTaxa Each plot is given one page. This is the maximum number of bars to include in each one-page plot. r_PlotEffectSize.disableCohensD Options: Y/N. If N (default), produce plots for binary attributes showing effect size calculated as Cohen's d. If Y, skip this plot type. r_PlotEffectSize.disableRSquared Options: Y/N. If N (default), produce plots showing effect size calculated as the r-squared value. If Y, skip this plot type. r_PlotEffectSize.disableFoldChange Options: Y/N. If N (default), produce plots for binary attributes showing the fold change. If Y, skip this plot type.","title":"r_PlotEffectSize"},{"location":"Configuration/#r_plotmds","text":"Property Description r_PlotMds.numAxis Sets # MDS axis to plot r_PlotMds.distance distance metric for calculating MDS (default: bray) r_PlotMds.reportFields Override field used to explicitly list metadata columns to build MDS plots. If left undefined, all columns are reported","title":"r_PlotMds"},{"location":"Configuration/#rarefyotucounts","text":"Property Description rarefyOtuCounts.iterations Positive integer. The number of iterations to randomly select the rarefyOtuCounts.quantile of OTUs rarefyOtuCounts.lowAbundantCutoff Minimum percentage of samples that must contain an OTU. rarefyOtuCounts.quantile Quantile for rarefication. The number of OTUs/sample are ordered, all samples with more OTUs than the quantile sample are subselected without replacement until they have the same number of OTUs as the quantile sample rarefyOtuCounts.rmLowSamples Options: Y/N. If Y, all samples below the rarefyOtuCounts.quantile quantile sample are removed","title":"rarefyOtuCounts"},{"location":"Configuration/#rarefyseqs","text":"Property Description rarefySeqs.max Randomly select maximum number of sequences per sample rarefySeqs.min Discard samples without minimum number of sequences","title":"rarefySeqs"},{"location":"Configuration/#rdp","text":"Property Description rdp.db File property used to define an alternate RDP database file rdp.jar File property for RDP java executable JAR rdp.minThresholdScore Required RDP minimum threshold score for valid OTUs","title":"rdp"},{"location":"Configuration/#report","text":"Property Description report.logBase Options: 10/e. If e, use natural log (base e), otherwise use log base 10 report.minCount Integer, minimum table count allowed. If a count less that this value is found, it is set to 0. report.numHits Options: Y/N. If Y, and add Num_Hits to metadata report.numReads Options: Y/N. If Y, and add Num_Reads to metadata report.scarceCountCutoff Minimum percentage of samples that must contain a count value for it to be kept. report.scarceSampleCutoff Minimum percentage of data columns that must be non-zero to keep the sample. report.taxonomyLevels Options: domain, phylum, class, order, family, genus, species. Generate reports for listed taxonomy levels","title":"report"},{"location":"Configuration/#script","text":"Property Description script.batchSize Number of sequence files to process per worker script script.defaultHeader Used to set shebang line to define scripts as bash executables, such as \"#!/bin/bash\" script.numThreads Integer value passed to any module that takes a number of threads parameter script.permissions Set chmod command security bits on generated scripts (Ex. 770) script.timeout Integer, time (minutes) before worker scripts times out.","title":"script"},{"location":"Configuration/#seqfilevalidator","text":"Property Description seqFileValidator.requireEqualNumPairs Options: Y/N. default Y. seqFileValidator.seqMaxLen maximum number of bases per read seqFileValidator.seqMinLen minimum number of bases per read","title":"seqFileValidator"},{"location":"Configuration/#trimprimers","text":"Property Description trimPrimers.filePath Path to file containing one primer sequence per line. trimPrimers.requirePrimer Options: Y/N. If Y, TrimPrimers will discard reads that do not include a primer sequence.","title":"trimPrimers"},{"location":"Configuration/#validation","text":"Property Description validation.compareOn Which columns in the expectation file should be used for the comparison. Options: name, size, md5. Default: use all columns in the expectation file. validation.disableValidation Turn off validation. No validation file output is produced. Options: Y/N. default: N validation.expectationFile File path to the table of expectations. If a directory is given, BioLockJ will look for a file named after the module being evaluated. validation.reportOn Which attributes of the file should be included in the validation report file. Options: name, size, md5 validation.sizeWithinPercent What percentage difference is permitted between an output file and its expectation. Options: any positive number validation.stopPipeline If enabled, the validation utlility will stop the pipeline if any module fails validation. Options: Y/N","title":"validation"},{"location":"Dependencies/","text":"BioLockJ requires Java 1.8+ and a Unix-like operating system such as Darwin/macOS . Dependencies are required by modules listed in the BioModule Function column. Users DO NOT NEED TO INSTALL dependencies if not interested in the listed modules. For example, if you intend to classify 16S samples with RDP and WGS samples with Kraken, do not install: Bowtie2, GNU Awk, GNU Gzip, MetaPhlAn2, Python, QIIME 1, or Vsearch. # Program Version BioModule Function Link 1 Bowtie2 2.3.2 Metaphlan2Classifier : Build reference indexes download 2 GNU Awk 4.0.2 AwkFastaConverter : Convert Fastq to Fasta BuildQiimeMapping : Format metadata as QIIME mapping QiimeClosedRefClassifier : Build batch mapping files download 3 GNU Gzip 1.5 AwkFastaConverter : Decompress .gz files Gunzipper : Decompress .gz files download 4 Kraken 0.10.5-beta KrakenClassifier : Report WGS taxonomic summary download 5 MetaPhlAn2 2.0 Metaphlan2Classifier : Report WGS taxonomic summary (WGS) download 6 Python 2.7.12 BuildQiimeMapping : Run validate_mapping_file.py MergeQiimeOtuTables : Run merge_otu_tables.py QiimeClosedRefClassifier : Run pick_closed_reference_otus.py QiimeDeNovoClassifier : Run pick_de_novo_otus.py QiimeOpenRefClassifier : Run pick_open_reference_otus.py QiimeClassifier : Run add_alpha_to_mapping_file.py, add_qiime_labels.py, alpha_diversity.py, filter_otus_from_otu_table.py, print_qiime_config.py, and summarize_taxa.py Metaphlan2Classifier : Run metaphlan2.py download 7 PEAR 0.9.8 Paired-End reAd merger PearMergeReads Merge paired Fastq files since some classifiers ( RDP & QIIME ) will not accept paired reads. download 8 QIIME 1 1.9.1 Quantitative Insights Into Microbial Ecology BuildQiimeMapping : Validate QIIME mapping MergeQiimeOtuTables : Merge otu_table.biom files QiimeClosedRefClassifier : Pick OTUs by reference QiimeDeNovoClassifier : Pick OTUs by clustering QiimeOpenRefClassifier : Pick OTUs by reference and clustering QiimeClassifier : Report 16S taxonomic summary download 9 R 3.5.0 R_CalculateStats : Statistical modeling R_PlotPvalHistograms : Plot p-value histograms for each reportable metadata field R_PlotOtus : Build OTU-metadata boxplots and scatterplots R_PlotMds : Plot by top MDS axis R_PlotEffectSize : Build barplot of effect magnetude by OTU/taxa download 10 R-coin 1.2 COnditional Inference procedures in a permutatioN test framework R_CalculateStats : Compute exact Wilcox_test p-values download 11 R-ggpubr 0.1.8 R_PlotPvalHistograms : Set color palette R_PlotMds : Set color palette R_PlotEffectSize : Set color palette download 12 R-Kendall 2.2 R_CalculateStats : Compute rank correlation p-values for continuous data types download 13 R-properties 0.0-9 R_Module : Reads in the MASTER configuration properties file from the pipeline root directory download 14 R-stringr 1.2.0 R_Module : For string manipulation for handling Configuration properties download 15 R-vegan 2.5-2 R_PlotMds : Ordination methods, diversity analysis and other functions for ecologists. download 16 RDP 2.12 Ribosomal Database Project RdpClassifier : Report 16S taxonomic summary download 17 Vsearch 2.4.3 QiimeDeNovoClassifier : Chimera detection QiimeOpenRefClassifier : Chimera detection download Version Dependencies The Version column contains the version tested during BioLockJ development, but other versions can often be substituted. Major releases (such as Python 2 vs. Python 3) contain API changes that will not integrate with the current BioLockJ code. Application APIs often change over time, so not all versions are supported. For example, Bowtie2 did not add the large index functionality until version 2.3.2.","title":"Dependencies"},{"location":"Dependencies/#version-dependencies","text":"The Version column contains the version tested during BioLockJ development, but other versions can often be substituted. Major releases (such as Python 2 vs. Python 3) contain API changes that will not integrate with the current BioLockJ code. Application APIs often change over time, so not all versions are supported. For example, Bowtie2 did not add the large index functionality until version 2.3.2.","title":"Version Dependencies"},{"location":"Example-Pipeline/","text":"In our example analysis, we investigate the differences between the microbiome of 20 rural and 20 recently urbanized subjects from the Chinese province of Hunan. For more information on this dataset, please review the analysis Fodor Lab published in the Sep 2017 issue of the journal Microbiome: https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-017-0338-7 Step 1: Prepare BioLockJ Config File The BioLockJ project Config chinaKrakenFullDB.properties lists 5 BioModules to run (lines 3-7) + 13 properties: #BioModule biolockj.module.implicit.RegisterNumReads #BioModule biolockj.module.classifier.wgs.KrakenClassifier #BioModule biolockj.module.report.taxa.NormalizeTaxaTables #BioModule biolockj.module.report.r.R_PlotPvalHistograms #BioModule biolockj.module.report.r.R_PlotOtus In addition to the 5 listed BioModules, 4 additional implicit BioModules will also run: Mod# Module Description 1 ImportMetadata Always run 1st (for all pipelines) 2 KrakenParser Always run after KrakenClassifier 3 AddMetadataToOtuTables Always run just before the 1st R module 4 CalculateStats Always run as the 1st R module. Key properties: Line# Property Description 08 cluster.jobHeader Each script will run on 1 node, 16 cores, and 128GB RAM for up to 30 minutes 10 pipeline.defaultProps Default config file defines most properties \u2013 in this case copperhead.properties 12 input.dirPaths Directory path containing 40 gzipped whole genome sequencing (WGS) fastq files 18 metadata.filePath Metadata file path: chinaMetadata.tsv BioLockJ must associate sequence files in input.dirPaths with the correct metadata row. This is done by matching sequence file names to the 1st column in the metadata file. If the Sample ID is not found in your file names, the file names must be updated. Use the following properties to ignore a file prefix or suffix when matching the sample IDs. input.suffixFw input.suffixRv input.trimPrefix input.trimSuffix Sample IDs from 1st column of the metadata file: 081A, 082A, 083A...etc. Sequence file names: 081A_R1.fq.gz, 082A_R1.fq.gz, 083A_R1.fq.gz...etc. The default Config file, copperhead.properties, has its own default Config file standard.properties which defines the property input.suffixFw=_R1 . As a result, all characters starting with (and including) \u201c_R1\u201d are ignored when matching the file name to the metadata sample ID. Step 2: Run BioLockJ Pipeline > biolockj ~/chinaKrakenFullDB.properties Look in the BioLockJ pipeline output directory defined by $BLJ_PROJ for a new pipeline directory named after the property file + today\u2019s date: ~/projects/chinaKrakenFullDB_2018Apr09 The 5 configured modules have run in order, with the addition of 2 implicit modules (1st and last) which are added to all pipelines automatically. The biolockjComplete file indicates the pipeline ran successfully. Step 3: Review Pipeline Summary Run the blj_summary command to review the pipeline execution summary. > blj_summary Pipeline Summary Step 4: Download R Reports Run the blj_download command to get the command needed to download the analysis. > blj_download > rsync Step 5: Analyze R Reports Open downloadDir on your local filesystem to review the analysis. This directory contains: Output Description /temp Directory where R log files are saved if R script runs locally. /tables Directory containing the OTU tables. /local Directory where R script output is saved if R script runs locally and r.debug=Y . *.RData The saved R sessions for R modules run if r.saveRData=Y . chinaKrakenFullDB.log The pipeline Java log file. MAIN_*.R Each R script for each module that generated reports has been updated to run on your local filesystem. *.tsv files Spreadsheets containing p-value and R^2 statistics for each OTU in the taxonomy level. *.pdf files P-value histograms, and bar-charts or scatterplots for each OTU in the taxonomy level. Each R module generates a report for each report.taxonomyLevel configured: Open chinaKrakenFullDB_Log10_genus.pdf The report begins with the unadjusted P-Value Distributions: Since r.numHistogramBreaks=20 so the 1st bar represents the p-values < 0.05. The ruralUrban attribute appears significant, as indicated by the high number p-values < 0.05. For each OTU, a bar-chart or scatterplot is output with adjusted parametric and non-parametric p-values formatted using in the plot header. The p-value format is defined by r.pValFormat . The p-adjust method is defined by rStats.pAdjustMethod . P-values that meet the r.pvalCutoff threshold are highlighted with r.colorHighlight .","title":"Example Pipeline"},{"location":"Example-Pipeline/#step-1-prepare-biolockj-config-file","text":"The BioLockJ project Config chinaKrakenFullDB.properties lists 5 BioModules to run (lines 3-7) + 13 properties: #BioModule biolockj.module.implicit.RegisterNumReads #BioModule biolockj.module.classifier.wgs.KrakenClassifier #BioModule biolockj.module.report.taxa.NormalizeTaxaTables #BioModule biolockj.module.report.r.R_PlotPvalHistograms #BioModule biolockj.module.report.r.R_PlotOtus In addition to the 5 listed BioModules, 4 additional implicit BioModules will also run: Mod# Module Description 1 ImportMetadata Always run 1st (for all pipelines) 2 KrakenParser Always run after KrakenClassifier 3 AddMetadataToOtuTables Always run just before the 1st R module 4 CalculateStats Always run as the 1st R module. Key properties: Line# Property Description 08 cluster.jobHeader Each script will run on 1 node, 16 cores, and 128GB RAM for up to 30 minutes 10 pipeline.defaultProps Default config file defines most properties \u2013 in this case copperhead.properties 12 input.dirPaths Directory path containing 40 gzipped whole genome sequencing (WGS) fastq files 18 metadata.filePath Metadata file path: chinaMetadata.tsv BioLockJ must associate sequence files in input.dirPaths with the correct metadata row. This is done by matching sequence file names to the 1st column in the metadata file. If the Sample ID is not found in your file names, the file names must be updated. Use the following properties to ignore a file prefix or suffix when matching the sample IDs. input.suffixFw input.suffixRv input.trimPrefix input.trimSuffix Sample IDs from 1st column of the metadata file: 081A, 082A, 083A...etc. Sequence file names: 081A_R1.fq.gz, 082A_R1.fq.gz, 083A_R1.fq.gz...etc. The default Config file, copperhead.properties, has its own default Config file standard.properties which defines the property input.suffixFw=_R1 . As a result, all characters starting with (and including) \u201c_R1\u201d are ignored when matching the file name to the metadata sample ID.","title":"Step 1: Prepare BioLockJ Config File"},{"location":"Example-Pipeline/#step-2-run-biolockj-pipeline","text":"> biolockj ~/chinaKrakenFullDB.properties Look in the BioLockJ pipeline output directory defined by $BLJ_PROJ for a new pipeline directory named after the property file + today\u2019s date: ~/projects/chinaKrakenFullDB_2018Apr09 The 5 configured modules have run in order, with the addition of 2 implicit modules (1st and last) which are added to all pipelines automatically. The biolockjComplete file indicates the pipeline ran successfully.","title":"Step 2: Run BioLockJ Pipeline"},{"location":"Example-Pipeline/#step-3-review-pipeline-summary","text":"Run the blj_summary command to review the pipeline execution summary. > blj_summary Pipeline Summary","title":"Step 3: Review Pipeline Summary"},{"location":"Example-Pipeline/#step-4-download-r-reports","text":"Run the blj_download command to get the command needed to download the analysis. > blj_download > rsync","title":"Step 4: Download R Reports"},{"location":"Example-Pipeline/#step-5-analyze-r-reports","text":"Open downloadDir on your local filesystem to review the analysis. This directory contains: Output Description /temp Directory where R log files are saved if R script runs locally. /tables Directory containing the OTU tables. /local Directory where R script output is saved if R script runs locally and r.debug=Y . *.RData The saved R sessions for R modules run if r.saveRData=Y . chinaKrakenFullDB.log The pipeline Java log file. MAIN_*.R Each R script for each module that generated reports has been updated to run on your local filesystem. *.tsv files Spreadsheets containing p-value and R^2 statistics for each OTU in the taxonomy level. *.pdf files P-value histograms, and bar-charts or scatterplots for each OTU in the taxonomy level. Each R module generates a report for each report.taxonomyLevel configured:","title":"Step 5: Analyze R Reports"},{"location":"Example-Pipeline/#open-chinakrakenfulldb_log10_genuspdf","text":"The report begins with the unadjusted P-Value Distributions: Since r.numHistogramBreaks=20 so the 1st bar represents the p-values < 0.05. The ruralUrban attribute appears significant, as indicated by the high number p-values < 0.05. For each OTU, a bar-chart or scatterplot is output with adjusted parametric and non-parametric p-values formatted using in the plot header. The p-value format is defined by r.pValFormat . The p-adjust method is defined by rStats.pAdjustMethod . P-values that meet the r.pvalCutoff threshold are highlighted with r.colorHighlight .","title":"Open chinaKrakenFullDB_Log10_genus.pdf"},{"location":"FAQ/","text":"Q: If biolockj indicates that my pipeline started successfully, but the pipeline root directory is not created, how do I debug the root cause of the failure? A: Generally, errors are output to the pipeline log file and documented in the notification email, but invalid configuration settings may cause a fatal error to occur before the pipeline directory is created. In this scenario, look in your $HOME directory for a file name that starts with \"biolockj_FATAL_ERROR_\". Verify you are running Java 1.8+ java -version Look in the error message found in $HOME/biolockj_FATAL_ERROR_* for a reference to one of your Config file parameters, the most common culprit is: pipeline.defaultProps $BLJ_PROJ misconfigured in /script/blj_config Q: How should I configure input properties for a demultiplexed dataset? A: Name the sequence files using the Sample IDs listed in your metadata file. Sequence file names containing a prefix or suffix (in addition the Sample ID) can be used as long as there is a unique character string that can be used to identify the boundary between the Sample ID and its prefix or suffix. These values can be set via the input.trimPrefix & input.trimSuffix properties. Set input.trimPrefix to a character string that precedes the sample ID for all samples Set input.trimSuffix to a character string that comes after the sample ID for all samples If a single prefix or suffix identifier cannot be used for all samples, the file names must be updated so that a universal prefix or suffix identifier can be used. Example Sample IDs = mbs1, mbs2, mbs3, mbs4 Example File names + gut_mbs1.fq.gz + gut_mbs2.fq.gz + oral_mbs3.fq + oral_mbs4.fq Config Properties + input.trimPrefix =_ + input.trimSuffix =.fq All characters before (and including) the 1st \"_\" in the file name are trimmed All characters after (and including) the 1st \".fq\" in the file name are trimmed BioLockJ automatically trims extensions \".fasta\" and \".fastq\" as if configured in input.trimSuffix . Q: How do I configure my pipeline for multiplexed data? A: BioLockJ automatically adds the Demultiplexer as the 2nd module - after ImportMetadata - when processing multiplexed data. The Demultiplexer requires that the sequence headers contain either the Sample ID or an identifying barcode. Optionally, the barcode can be contained in the sequence itself. If your data does not conform to one of the following scenarios you will need to pre-process your sequence data to conform to a valid format. If samples are not identified by sample ID in the sequence headers: Set demux.strategy =id_in_header Set input.trimPrefix to a character string that precedes the sample ID for all samples . Set input.trimSuffix to a character string that comes after the sample ID for all samples . Sample IDs = mbs1, mbs2, mbs3, mbs4 Scenario 1: Your multiplexed files include Sample IDs in the fastq sequence headers @mbs1_134_M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0 @mbs2_12_M02825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0 @mbs3_551_M03825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0 @mbs4_1234_M04825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0 Required Config + input.trimPrefix =@ + input.trimSuffix =_ All characters before (and including) the 1st \"@\" in the sequence header are trimmed All characters after (and including) the 1st \"_\" in the sequence header are trimmed If samples are identified by barcode (in the header or sequence): Set demux.strategy =barcode_in_header or demux.strategy =barcode_in_seq Set metadata.filePath to metadata file path. Set metadata.barcodeColumn to the barcode column name. If the metadata barcodes are listed as reverse compliments, set demux.barcodeUseReverseCompliment =Y. The metadata file must be prepared by adding a unique sequence barcode in the metadata.barcodeColumn column. This information is often available in a mapping file provided by the sequencing center that produced the raw data. Metadata file ID BarcodeColumn mbs1 GAGGCATGACTGGATA mbs2 NAGGCATATTTGCACA mbs3 GACCCATGACTGCATA mbs4 TACCCAGCACCGCTTA Scenario 2: Your multiplexed files include a barcode in the headers @M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0:GAGGCATGACTGGATA @M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0:NAGGCATATTTGCACA @M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0:GACCCATGACTGCATA @M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA Required Config + demux.strategy =barcode_in_header + metadata.barcodeColumn =BarcodeColumn + metadata.filePath = Scenario 3: Your multiplexed files include a barcode in the sequences >M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0: GAGGCATGACTGGATATATACATACTGAGGCATGACTACTTACTATAAGGCTTACTGACTGGTTACTGACTGGGAGGCATGACTACTTACTATAA >M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0: CAGGCATATTTGCACACTAGAGGCAAGTTACTGACTGGATATACTGAGGCATGGGAGGCATGACTCTATAAGGCTTACTGACTGGTTACTGACTG >M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0: CCATGAGACCTGCATA CCATGAGACCTGCATACACTGTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGGCT >M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA TACCCAGCACCGCTTCCTTGACTTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGG","title":"FAQ"},{"location":"FAQ/#q-if-biolockj-indicates-that-my-pipeline-started-successfully-but-the-pipeline-root-directory-is-not-created-how-do-i-debug-the-root-cause-of-the-failure","text":"A: Generally, errors are output to the pipeline log file and documented in the notification email, but invalid configuration settings may cause a fatal error to occur before the pipeline directory is created. In this scenario, look in your $HOME directory for a file name that starts with \"biolockj_FATAL_ERROR_\". Verify you are running Java 1.8+ java -version Look in the error message found in $HOME/biolockj_FATAL_ERROR_* for a reference to one of your Config file parameters, the most common culprit is: pipeline.defaultProps $BLJ_PROJ misconfigured in /script/blj_config","title":"Q: If biolockj indicates that my pipeline started successfully, but the pipeline root directory is not created, how do I debug the root cause of the failure?"},{"location":"FAQ/#q-how-should-i-configure-input-properties-for-a-demultiplexed-dataset","text":"A: Name the sequence files using the Sample IDs listed in your metadata file. Sequence file names containing a prefix or suffix (in addition the Sample ID) can be used as long as there is a unique character string that can be used to identify the boundary between the Sample ID and its prefix or suffix. These values can be set via the input.trimPrefix & input.trimSuffix properties. Set input.trimPrefix to a character string that precedes the sample ID for all samples Set input.trimSuffix to a character string that comes after the sample ID for all samples If a single prefix or suffix identifier cannot be used for all samples, the file names must be updated so that a universal prefix or suffix identifier can be used.","title":"Q: How should I configure input properties for a demultiplexed dataset?"},{"location":"FAQ/#example","text":"Sample IDs = mbs1, mbs2, mbs3, mbs4 Example File names + gut_mbs1.fq.gz + gut_mbs2.fq.gz + oral_mbs3.fq + oral_mbs4.fq Config Properties + input.trimPrefix =_ + input.trimSuffix =.fq All characters before (and including) the 1st \"_\" in the file name are trimmed All characters after (and including) the 1st \".fq\" in the file name are trimmed BioLockJ automatically trims extensions \".fasta\" and \".fastq\" as if configured in input.trimSuffix .","title":"Example"},{"location":"FAQ/#q-how-do-i-configure-my-pipeline-for-multiplexed-data","text":"A: BioLockJ automatically adds the Demultiplexer as the 2nd module - after ImportMetadata - when processing multiplexed data. The Demultiplexer requires that the sequence headers contain either the Sample ID or an identifying barcode. Optionally, the barcode can be contained in the sequence itself. If your data does not conform to one of the following scenarios you will need to pre-process your sequence data to conform to a valid format.","title":"Q: How do I configure my pipeline for multiplexed data?"},{"location":"FAQ/#if-samples-are-not-identified-by-sample-id-in-the-sequence-headers","text":"Set demux.strategy =id_in_header Set input.trimPrefix to a character string that precedes the sample ID for all samples . Set input.trimSuffix to a character string that comes after the sample ID for all samples . Sample IDs = mbs1, mbs2, mbs3, mbs4 Scenario 1: Your multiplexed files include Sample IDs in the fastq sequence headers @mbs1_134_M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0 @mbs2_12_M02825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0 @mbs3_551_M03825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0 @mbs4_1234_M04825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0 Required Config + input.trimPrefix =@ + input.trimSuffix =_ All characters before (and including) the 1st \"@\" in the sequence header are trimmed All characters after (and including) the 1st \"_\" in the sequence header are trimmed","title":"If samples are not identified by sample ID in the sequence headers:"},{"location":"FAQ/#if-samples-are-identified-by-barcode-in-the-header-or-sequence","text":"Set demux.strategy =barcode_in_header or demux.strategy =barcode_in_seq Set metadata.filePath to metadata file path. Set metadata.barcodeColumn to the barcode column name. If the metadata barcodes are listed as reverse compliments, set demux.barcodeUseReverseCompliment =Y. The metadata file must be prepared by adding a unique sequence barcode in the metadata.barcodeColumn column. This information is often available in a mapping file provided by the sequencing center that produced the raw data. Metadata file ID BarcodeColumn mbs1 GAGGCATGACTGGATA mbs2 NAGGCATATTTGCACA mbs3 GACCCATGACTGCATA mbs4 TACCCAGCACCGCTTA Scenario 2: Your multiplexed files include a barcode in the headers @M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0:GAGGCATGACTGGATA @M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0:NAGGCATATTTGCACA @M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0:GACCCATGACTGCATA @M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA Required Config + demux.strategy =barcode_in_header + metadata.barcodeColumn =BarcodeColumn + metadata.filePath = Scenario 3: Your multiplexed files include a barcode in the sequences >M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0: GAGGCATGACTGGATATATACATACTGAGGCATGACTACTTACTATAAGGCTTACTGACTGGTTACTGACTGGGAGGCATGACTACTTACTATAA >M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0: CAGGCATATTTGCACACTAGAGGCAAGTTACTGACTGGATATACTGAGGCATGGGAGGCATGACTCTATAAGGCTTACTGACTGGTTACTGACTG >M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0: CCATGAGACCTGCATA CCATGAGACCTGCATACACTGTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGGCT >M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA TACCCAGCACCGCTTCCTTGACTTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGG","title":"If samples are identified by barcode (in the header or sequence):"},{"location":"Failure-Recovery/","text":"Failed Pipelines Failed pipelines can be restarted to save the progress made by successfully completed modules. To restart a failed pipeline, add -r parameter: biolockj -r <pipeline root> Check the BioLockJ execution summary after any failure occurs via the blj_summary command. blj_summary Review the BioLockJ log file in your pipeline root directory. This is the most complete source of information on pipeline execution and may contain useful error messages that help resolve the root cause of the failure. If your pipeline directory is not created, you likely have invalid file paths or missing/invalid properties Check the FATAL_ERROR file your $HOME directory. This is where BioLockJ saves error messages for failures occurring prior to the creation of your pipeline root directory. cat \"$HOME/biolockj_FATAL_ERROR_*\" The most common culprits are: pipeline.defaultProps Missing/invalid $BLJ_PROJ directory set by the install script echo $BLJ_PROJ Failures that occur in a module/script are not logged to the Java log file since these run outside of the Java application. The failed module directory may have an indicator file that gives either the sample ID or bash script line that failed. If the failed module was running a bash script on the cluster, check the module/qsub directory for the cluster job output and error files which may contain additional information.","title":"Failure Recovery"},{"location":"Failure-Recovery/#failed-pipelines","text":"Failed pipelines can be restarted to save the progress made by successfully completed modules. To restart a failed pipeline, add -r parameter: biolockj -r <pipeline root> Check the BioLockJ execution summary after any failure occurs via the blj_summary command. blj_summary Review the BioLockJ log file in your pipeline root directory. This is the most complete source of information on pipeline execution and may contain useful error messages that help resolve the root cause of the failure. If your pipeline directory is not created, you likely have invalid file paths or missing/invalid properties Check the FATAL_ERROR file your $HOME directory. This is where BioLockJ saves error messages for failures occurring prior to the creation of your pipeline root directory. cat \"$HOME/biolockj_FATAL_ERROR_*\" The most common culprits are: pipeline.defaultProps Missing/invalid $BLJ_PROJ directory set by the install script echo $BLJ_PROJ Failures that occur in a module/script are not logged to the Java log file since these run outside of the Java application. The failed module directory may have an indicator file that gives either the sample ID or bash script line that failed. If the failed module was running a bash script on the cluster, check the module/qsub directory for the cluster job output and error files which may contain additional information.","title":"Failed Pipelines"},{"location":"Getting-Started/","text":"Installation and test Basic installation The basic installation assumes a unix-like environment and a bash shell. To see what shell you currently using, run echo $0 . If you are not in a bash shell, you can change your current session to a bash shell, run chsh -s /bin/bash . 1. Download the latest release & unpack the tarball. tar -zxf BioLockJ-v1.2.8.tar.gz Save the folder wherever you like to keep executables. If you choose to download the source code, you will need to compile it by running ant with the build.xml file in the resources folder. 2. Run the install script The install script updates the $USER bash profile to call blj_config . See Commands for a full description of blj_config cd BioLockJ* ./install # Saved backup: /users/joe/.bash_profile~ # Saved profile: /users/joe/.bash_profile # BioLockJ installation complete! This will add the required variables to your path when you start your next session. exit # exit and start a new session Start a new bash session and verify that biolockj is on your $PATH . A new terminal window or a fresh log in will start a new session. biolockj --version biolockj --help 3. Run the test pipeline echo $BLJ # /path/to/BioLockJ biolockj ${BLJ}/templates/myFirstPipeline/myFirstPipeline.properties # Initializing BioLockJ.. # Building pipeline: /Users/joe/apps/BioLockJ/pipelines/myFirstPipeline_2020Jan17 # blj_go -> Move to pipeline output directory # blj_log -> Tail pipeline log (accepts tail runtime parameters) # blj_summary -> View module execution summary # Fetching pipeline status # # Pipeline is complete. Notice the use of the $BLJ variable. This variable is created by the installation process; it points to the BioLockJ folder. The myFirstPipeline project is the first in the tutorial series designed to introduce new users to the layout of a BioLockJ pipeline. You should take a moment to review your first pipeline . Docker installation 1. Install docker See the current instructions for installing docker on your system: https://docs.docker.com/get-started/ You'll need to be able to run the docker hello world example: docker run hello-world # Hello from Docker! # This message shows that your installation appears to be working correctly. 2. Turn on file sharing Depending on your system and docker installation, this may be on by default. File sharing, also called volume sharing, is what allows programs inside docker containers to interact with files stored on your computer. Dependong on your version of Docker Desktop, this setting may be under Docker > Prefernces > File Sharing , or Preferences > Resources > File Sharing or something similar. Make sure this feature is enabled. Any file that must be read by any part of the BioLockJ pipeline must be under one of the share-enabled folders. The BioLockJ Projects directory (BLJ_PROJ) must also be under one of these share-enabled folders. 3. Download and install BioLockj Follow the download and install steps in the Basic Installation instructions. 4. Run the test pipeline in docker biolockj --docker --blj ${BLJ}/templates/myFirstPipeline/myFirstPipeline.properties # # Created \"/Users/ieclabau/runDockerClone.sh\" # This script will launch another instance of this docker image, # with the same env vars + volumes, but in interactive mode. # # Docker container id: 336259e7d3b8d9ab2fa71202258b562664be1bf9645d503a790ae5e9da15ce97 # Initializing BioLockJ.. # Building pipeline: /Users/joe/apps/BioLockJ/pipelines/myFirstPipeline_2020Jan17 # blj_go -> Move to pipeline output directory # blj_log -> Tail pipeline log (accepts tail runtime parameters) # blj_summary -> View module execution summary # Fetching pipeline status # # Pipeline is complete. You should take a moment to review your first pipeline . Cluster installation Installing BioLockJ on a cluster follows the same process as the Basic Installation . EACH USER must run the install script in order to run the BioLockJ launch scripts. Use the property pipeline.env=cluster in your pipeline configuration to take advantage of parallell computing through the cluster. Review your first pipeline The variable $BLJ_PROJ points to your projects folder. See a list of all of the pipelines in your projects folder. ls $BLJ_PROJ By default, $BLJ_PROJ is set to the \"pipelines\" folder in BioLockJ ( $BLJ/pipelines ). To change this, add a line to your bash_profile (or equivilent file): export BLJ_PROJ=/path/to/my/projects . This line must be after the call to the blj_config script. Look at your most recent pipeline: blj_go This folder represents the analysis pipeline that you launched when you called biolockj on the file ${BLJ}/templates/myFirstPipeline/myFirstPipeline.properties . Notice that the original configuration (\"config\") file has been copied to this folder. Review the config file that was used to launch this pipeline: cat myFirstPipeline.properties Notice that modules are specified in the config using the keyword #BioModule . Each module in the pipeline creates a folder in the pipeline directory. Notice that an additional module \"00_ImportMetaData\" was added automatically. At the top level of the pipeline we see an empty flag file \"biolockjComplete\" which indicates that the pipeline finished successfully. While the pipeline is still in progress, the flag is \"biolockjStarted\"; and if the pipeline stopps due to an error, the flag is \"biolockjFailed\". The summary.txt file is a summary of each module as it ran during pipeline execution. This is the best place to start when reviewing a pipeline. The file \"MASTER_myFirstPipeline_<DATE>.properties\" is the complete list of all properties used during this pipeline. This includes properties that were set in the primary config file (\"myFirstPipeline.properties\"), and properties that are set as defaults in the BioLockJ program, and properties that are set in user-supplied default config files, which are specified in the primary config file using the pipeline.defaultProps= property. This \"MASTER_*.properties\" file contains all of the settings required to reproduce this pipeline. If the pipeline was run using docker, a file named dockerInfo.json will show the container information. The pipeline log file \"myFirstPipeline_<DATE>.log\" is an excellend resource for troubleshooting. The validation has tables recording the MD5 sum for each output from each module. If the pipeline is run again, this folder can be used to determine if the results in the new run are an exact match for this run. Within each module's folder, we see the \"biolockjComplete\" flag (the same flags are used in modules and at the top level). All output-procuing modules have a subfolder called output . Most modules also have folders script and temp . The output folder is used as input to down-stream modules. Modules are the building blocks of pipelines. For more information about modules, see Built-in BioModules . Making your own pipline Now that you have a working example, you can make your own pipeline. You may want to modify the example above, or look at others under /templates . Things are seldom perfect the first time. Its safe to assume you will make iterative changes to your pipeline configuration. BioLockJ offers some tools to facilitate this process. Check your pipeline using precheck mode Add modules onto your partial pipeline using restart Look through the base set of modules and even create your own A recommended practice is to make a subset of your data, and use that to develope your pipeline. Other notes for starting out Install any/all software Dependencies required by the modules you wish to include in your pipeline. BioLockJ is a pipeline manager, desigend to integrate and manage external tools. These external tools are not packaged into the BioLockJ program. BioLockJ must run in an environment where these other tools have been installed OR run through docker using the docker images that have the tools installed. The core program, and all modules packaged with it, have corresponding docker images. Notes about environments The main BioLockJ program can be used in these environments: a local machine with a unix-like system any machine running docker * a cluster, running a scheduler such as torque AWS cloud computing * (* The launch scripts will still be run from your local machine.) The launch process requires a unix-like environment. This includes linux, macOS, or an ubuntu environment running on Windows. If using docker , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run within the biolockj_controller container. If using AWS , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run on AWS. AWS is currently experimental. To ty it, see Working in Pure Docker If you are using BioLockJ on a shared system where another user has already installed BioLockJ, you will need to run the install script to create the required variables in your own user profile. There is also the option to run purely in docker, without installing even the launch scripts on your local machine. However this is considered a nitch case scenario and not well supported. For more information about the how/why to use each environment, see Supported Environments Shutting down a pipeline BioLockJ will shut down appropriately on its own when a pipeline either completes or fails. Sometimes , it is necessary to shut down the program pre-maturely. This is not an ideal exit and the steps depend on your environment. The main program is terminated by killing the java process. Any worker-processes that are still in progress will need to be shut down directly (or allowed to time out). To kill the BioLockJ program on a local system, get the id of the java process and kill it: ps # PID TTY TIME CMD # 1776 pts/0 00:00:00 bash # 1728 pts/0 00:00:00 ps # 4437 pts/0 00:00:00 java kill 4437 On a local system, workers are under the main program. To kill the BioLockJ program running in docker, get the ID of the docker container and use docker stop . docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES # f55a39311eb5 ubuntu \"/bin/bash\" 16 minutes ago Up 16 minutes brave_cori docker stop f55a39311eb5 In a docker pipeline, the container IDs for workers will also appear under ps. If you need to distinguish the BioLockJ containers from other docker containers running on your machine, you see a list of them in the current modules script directory in a file named MAIN*.sh_Started . To kill the BioLockJ program that is run in the foreground (ie, the -f arg was used), then killing the current process will kill the program. This is usually done with ctrl + c . To kill the BioLockJ program on a cluster environment, use kill just like the local case to stop a process on the head node, and use qdel (or the equivilent on your scheduler) to terminate workers running on compute nodes.","title":"Getting Started"},{"location":"Getting-Started/#installation-and-test","text":"","title":"Installation and test"},{"location":"Getting-Started/#basic-installation","text":"The basic installation assumes a unix-like environment and a bash shell. To see what shell you currently using, run echo $0 . If you are not in a bash shell, you can change your current session to a bash shell, run chsh -s /bin/bash .","title":"Basic installation"},{"location":"Getting-Started/#1-download-the-latest-release-unpack-the-tarball","text":"tar -zxf BioLockJ-v1.2.8.tar.gz Save the folder wherever you like to keep executables. If you choose to download the source code, you will need to compile it by running ant with the build.xml file in the resources folder.","title":"1. Download the latest release &amp; unpack the tarball."},{"location":"Getting-Started/#2-run-the-install-script","text":"The install script updates the $USER bash profile to call blj_config . See Commands for a full description of blj_config cd BioLockJ* ./install # Saved backup: /users/joe/.bash_profile~ # Saved profile: /users/joe/.bash_profile # BioLockJ installation complete! This will add the required variables to your path when you start your next session. exit # exit and start a new session Start a new bash session and verify that biolockj is on your $PATH . A new terminal window or a fresh log in will start a new session. biolockj --version biolockj --help","title":"2. Run the install script"},{"location":"Getting-Started/#3-run-the-test-pipeline","text":"echo $BLJ # /path/to/BioLockJ biolockj ${BLJ}/templates/myFirstPipeline/myFirstPipeline.properties # Initializing BioLockJ.. # Building pipeline: /Users/joe/apps/BioLockJ/pipelines/myFirstPipeline_2020Jan17 # blj_go -> Move to pipeline output directory # blj_log -> Tail pipeline log (accepts tail runtime parameters) # blj_summary -> View module execution summary # Fetching pipeline status # # Pipeline is complete. Notice the use of the $BLJ variable. This variable is created by the installation process; it points to the BioLockJ folder. The myFirstPipeline project is the first in the tutorial series designed to introduce new users to the layout of a BioLockJ pipeline. You should take a moment to review your first pipeline .","title":"3. Run the test pipeline"},{"location":"Getting-Started/#docker-installation","text":"","title":"Docker installation"},{"location":"Getting-Started/#1-install-docker","text":"See the current instructions for installing docker on your system: https://docs.docker.com/get-started/ You'll need to be able to run the docker hello world example: docker run hello-world # Hello from Docker! # This message shows that your installation appears to be working correctly.","title":"1. Install docker"},{"location":"Getting-Started/#2-turn-on-file-sharing","text":"Depending on your system and docker installation, this may be on by default. File sharing, also called volume sharing, is what allows programs inside docker containers to interact with files stored on your computer. Dependong on your version of Docker Desktop, this setting may be under Docker > Prefernces > File Sharing , or Preferences > Resources > File Sharing or something similar. Make sure this feature is enabled. Any file that must be read by any part of the BioLockJ pipeline must be under one of the share-enabled folders. The BioLockJ Projects directory (BLJ_PROJ) must also be under one of these share-enabled folders.","title":"2. Turn on file sharing"},{"location":"Getting-Started/#3-download-and-install-biolockj","text":"Follow the download and install steps in the Basic Installation instructions.","title":"3. Download and install BioLockj"},{"location":"Getting-Started/#4-run-the-test-pipeline-in-docker","text":"biolockj --docker --blj ${BLJ}/templates/myFirstPipeline/myFirstPipeline.properties # # Created \"/Users/ieclabau/runDockerClone.sh\" # This script will launch another instance of this docker image, # with the same env vars + volumes, but in interactive mode. # # Docker container id: 336259e7d3b8d9ab2fa71202258b562664be1bf9645d503a790ae5e9da15ce97 # Initializing BioLockJ.. # Building pipeline: /Users/joe/apps/BioLockJ/pipelines/myFirstPipeline_2020Jan17 # blj_go -> Move to pipeline output directory # blj_log -> Tail pipeline log (accepts tail runtime parameters) # blj_summary -> View module execution summary # Fetching pipeline status # # Pipeline is complete. You should take a moment to review your first pipeline .","title":"4. Run the test pipeline in docker"},{"location":"Getting-Started/#cluster-installation","text":"Installing BioLockJ on a cluster follows the same process as the Basic Installation . EACH USER must run the install script in order to run the BioLockJ launch scripts. Use the property pipeline.env=cluster in your pipeline configuration to take advantage of parallell computing through the cluster.","title":"Cluster installation"},{"location":"Getting-Started/#review-your-first-pipeline","text":"The variable $BLJ_PROJ points to your projects folder. See a list of all of the pipelines in your projects folder. ls $BLJ_PROJ By default, $BLJ_PROJ is set to the \"pipelines\" folder in BioLockJ ( $BLJ/pipelines ). To change this, add a line to your bash_profile (or equivilent file): export BLJ_PROJ=/path/to/my/projects . This line must be after the call to the blj_config script. Look at your most recent pipeline: blj_go This folder represents the analysis pipeline that you launched when you called biolockj on the file ${BLJ}/templates/myFirstPipeline/myFirstPipeline.properties . Notice that the original configuration (\"config\") file has been copied to this folder. Review the config file that was used to launch this pipeline: cat myFirstPipeline.properties Notice that modules are specified in the config using the keyword #BioModule . Each module in the pipeline creates a folder in the pipeline directory. Notice that an additional module \"00_ImportMetaData\" was added automatically. At the top level of the pipeline we see an empty flag file \"biolockjComplete\" which indicates that the pipeline finished successfully. While the pipeline is still in progress, the flag is \"biolockjStarted\"; and if the pipeline stopps due to an error, the flag is \"biolockjFailed\". The summary.txt file is a summary of each module as it ran during pipeline execution. This is the best place to start when reviewing a pipeline. The file \"MASTER_myFirstPipeline_<DATE>.properties\" is the complete list of all properties used during this pipeline. This includes properties that were set in the primary config file (\"myFirstPipeline.properties\"), and properties that are set as defaults in the BioLockJ program, and properties that are set in user-supplied default config files, which are specified in the primary config file using the pipeline.defaultProps= property. This \"MASTER_*.properties\" file contains all of the settings required to reproduce this pipeline. If the pipeline was run using docker, a file named dockerInfo.json will show the container information. The pipeline log file \"myFirstPipeline_<DATE>.log\" is an excellend resource for troubleshooting. The validation has tables recording the MD5 sum for each output from each module. If the pipeline is run again, this folder can be used to determine if the results in the new run are an exact match for this run. Within each module's folder, we see the \"biolockjComplete\" flag (the same flags are used in modules and at the top level). All output-procuing modules have a subfolder called output . Most modules also have folders script and temp . The output folder is used as input to down-stream modules. Modules are the building blocks of pipelines. For more information about modules, see Built-in BioModules .","title":"Review your first pipeline"},{"location":"Getting-Started/#making-your-own-pipline","text":"Now that you have a working example, you can make your own pipeline. You may want to modify the example above, or look at others under /templates . Things are seldom perfect the first time. Its safe to assume you will make iterative changes to your pipeline configuration. BioLockJ offers some tools to facilitate this process. Check your pipeline using precheck mode Add modules onto your partial pipeline using restart Look through the base set of modules and even create your own A recommended practice is to make a subset of your data, and use that to develope your pipeline.","title":"Making your own pipline"},{"location":"Getting-Started/#other-notes-for-starting-out","text":"Install any/all software Dependencies required by the modules you wish to include in your pipeline. BioLockJ is a pipeline manager, desigend to integrate and manage external tools. These external tools are not packaged into the BioLockJ program. BioLockJ must run in an environment where these other tools have been installed OR run through docker using the docker images that have the tools installed. The core program, and all modules packaged with it, have corresponding docker images.","title":"Other notes for starting out"},{"location":"Getting-Started/#notes-about-environments","text":"The main BioLockJ program can be used in these environments: a local machine with a unix-like system any machine running docker * a cluster, running a scheduler such as torque AWS cloud computing * (* The launch scripts will still be run from your local machine.) The launch process requires a unix-like environment. This includes linux, macOS, or an ubuntu environment running on Windows. If using docker , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run within the biolockj_controller container. If using AWS , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run on AWS. AWS is currently experimental. To ty it, see Working in Pure Docker If you are using BioLockJ on a shared system where another user has already installed BioLockJ, you will need to run the install script to create the required variables in your own user profile. There is also the option to run purely in docker, without installing even the launch scripts on your local machine. However this is considered a nitch case scenario and not well supported. For more information about the how/why to use each environment, see Supported Environments","title":"Notes about environments"},{"location":"Getting-Started/#shutting-down-a-pipeline","text":"BioLockJ will shut down appropriately on its own when a pipeline either completes or fails. Sometimes , it is necessary to shut down the program pre-maturely. This is not an ideal exit and the steps depend on your environment. The main program is terminated by killing the java process. Any worker-processes that are still in progress will need to be shut down directly (or allowed to time out). To kill the BioLockJ program on a local system, get the id of the java process and kill it: ps # PID TTY TIME CMD # 1776 pts/0 00:00:00 bash # 1728 pts/0 00:00:00 ps # 4437 pts/0 00:00:00 java kill 4437 On a local system, workers are under the main program. To kill the BioLockJ program running in docker, get the ID of the docker container and use docker stop . docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES # f55a39311eb5 ubuntu \"/bin/bash\" 16 minutes ago Up 16 minutes brave_cori docker stop f55a39311eb5 In a docker pipeline, the container IDs for workers will also appear under ps. If you need to distinguish the BioLockJ containers from other docker containers running on your machine, you see a list of them in the current modules script directory in a file named MAIN*.sh_Started . To kill the BioLockJ program that is run in the foreground (ie, the -f arg was used), then killing the current process will kill the program. This is usually done with ctrl + c . To kill the BioLockJ program on a cluster environment, use kill just like the local case to stop a process on the head node, and use qdel (or the equivilent on your scheduler) to terminate workers running on compute nodes.","title":"Shutting down a pipeline"},{"location":"Pure-Docker/","text":"Pure Docker (experimental) This option is still in the early experimental stages. If you are running from any system that supports docker, you can run all commands through docker containers. 1. Install docker and make sure the hello world image runs correctly. (see above) 2. Enable file sharing. (see above) 3. Create a directory where your pipelines will be saved; this directory will be referred to as $BLJ_PROJ and it must be under a share-enabled directory. mkdir pipelines BLJ_PROJ=`pwd`/pipelines 4. Run the test pipeline in pure docker To test your all-docker BioLockJ, run: docker run --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $BLJ_PROJ:/mnt/efs/pipelines \\ biolockjdevteam/biolockj_controller:v1.2.7 \\ biolockj -df /app/biolockj/templates/myFirstPipeline/myFirstPipeline.properties You should see a pipeline named myFirstPipeline_<DATE> in your $BLJ_PROJ folder. The above example uses a config file that is in the docker container. To use a file from your own machine, mount the parent directory, and use the in-container path to reach it. To test this, extract the myFirstPipeline.properties file from the docker container and use it from the outside: ID=$(docker run -d biolockjdevteam/biolockj_controller:v1.2.7 /bin/bash) docker cp $ID:/app/biolockj/templates/myFirstPipeline/myFirstPipeline.properties ~/Downloads/origTest.properties Alternatively, create a file, and copy this text into it: #BioModule biolockj.module.implicit.RegisterNumReads #BioModule biolockj.module.seq.RarefySeqs #BioModule biolockj.module.seq.Multiplexer input.dirPaths=${BLJ}/templates/myFirstPipeline/rhizosphere_16S_data rarefySeqs.max=500 Run biolockj using this config file. docker run --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -e HOME=~ \\ biolockjdevteam/biolockj_controller:v1.2.7 \\ biolockj -df --blj_proj $BLJ_PROJ /Downloads/origTest.properties This should make a new pipeline called origTest_<DATE> in your $BLJ_PROJ folder. Finally, run the container interactively: docker run --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $BLJ_PROJ:/mnt/efs/pipelines \\ -it biolockjdevteam/biolockj_controller:v1.2.7 /bin/bash A general difficulty of the Pure Docker path is managing the volume mounts. Calling biolockj locally allows the biolockj script to handle this for you. Variable path support may make this eaiser. (TODO: add link to config variable descriptions)","title":"Pure Docker"},{"location":"Pure-Docker/#pure-docker-experimental","text":"This option is still in the early experimental stages. If you are running from any system that supports docker, you can run all commands through docker containers.","title":"Pure Docker (experimental)"},{"location":"Pure-Docker/#1-install-docker-and-make-sure-the-hello-world-image-runs-correctly-see-above","text":"","title":"1. Install docker and make sure the hello world image runs correctly. (see above)"},{"location":"Pure-Docker/#2-enable-file-sharing-see-above","text":"","title":"2. Enable file sharing. (see above)"},{"location":"Pure-Docker/#3-create-a-directory-where-your-pipelines-will-be-saved-this-directory-will-be-referred-to-as-blj_proj-and-it-must-be-under-a-share-enabled-directory","text":"mkdir pipelines BLJ_PROJ=`pwd`/pipelines","title":"3. Create a directory where your pipelines will be saved; this directory will be referred to as $BLJ_PROJ and it must be under a share-enabled directory."},{"location":"Pure-Docker/#4-run-the-test-pipeline-in-pure-docker","text":"To test your all-docker BioLockJ, run: docker run --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $BLJ_PROJ:/mnt/efs/pipelines \\ biolockjdevteam/biolockj_controller:v1.2.7 \\ biolockj -df /app/biolockj/templates/myFirstPipeline/myFirstPipeline.properties You should see a pipeline named myFirstPipeline_<DATE> in your $BLJ_PROJ folder. The above example uses a config file that is in the docker container. To use a file from your own machine, mount the parent directory, and use the in-container path to reach it. To test this, extract the myFirstPipeline.properties file from the docker container and use it from the outside: ID=$(docker run -d biolockjdevteam/biolockj_controller:v1.2.7 /bin/bash) docker cp $ID:/app/biolockj/templates/myFirstPipeline/myFirstPipeline.properties ~/Downloads/origTest.properties Alternatively, create a file, and copy this text into it: #BioModule biolockj.module.implicit.RegisterNumReads #BioModule biolockj.module.seq.RarefySeqs #BioModule biolockj.module.seq.Multiplexer input.dirPaths=${BLJ}/templates/myFirstPipeline/rhizosphere_16S_data rarefySeqs.max=500 Run biolockj using this config file. docker run --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -e HOME=~ \\ biolockjdevteam/biolockj_controller:v1.2.7 \\ biolockj -df --blj_proj $BLJ_PROJ /Downloads/origTest.properties This should make a new pipeline called origTest_<DATE> in your $BLJ_PROJ folder. Finally, run the container interactively: docker run --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $BLJ_PROJ:/mnt/efs/pipelines \\ -it biolockjdevteam/biolockj_controller:v1.2.7 /bin/bash A general difficulty of the Pure Docker path is managing the volume mounts. Calling biolockj locally allows the biolockj script to handle this for you. Variable path support may make this eaiser. (TODO: add link to config variable descriptions)","title":"4. Run the test pipeline in pure docker"},{"location":"Supported-Environments/","text":"What environements are supported The main BioLockJ program can be used in these environments: a local machine with a unix-like system any machine running docker * a cluster, running a scheduler like torque AWS cloud computing * (* The launch scripts will still be run from your local machine.) AWS is currently experimental. To ty it, see Working in Pure Docker In all cases, the launch process requires a unix-like environment. This includes linux, macOS, or an ubuntu environment running on Windows. There is also the option to run purely in docker, without installing even the launch scripts on your local machine. However this is considered a nitch case scenario and not well supported. Choosing an environment The major resources that come together in a pipeline are: data (project data and reference data) compute resources (mem, ram, cpu) key executables In theory, you could install all the tools you need on your laptop; put your data on your laptop, and run your whole analysis on your laptop. This would be a \"local\" pipeline; a single compute node is handling everything. However, in practice, a single machine typically doesn't have enough compute resources to run a modern bioinformatics pipeline in a realistic time frame; and the tools may be difficult to install, or even impossible to install on a given system. Docker provides key executables by packaging them into containers. After the initial hurdle of installing docker itself, the 'install' of executables that are available in docker images is trivial, and they produce very consistent results; even when different steps in your pipeline have conflicting system requirements. The underlying tools for all modules packaged with the main BioLockJ program are available via docker containers. Docker is the most recommended way to run a pipeline. However, these executables still have to come together with some compute resources. A computer cluster offers large amounts of compute resources and plenty of storage. Some clusters also have adminstrators (or other users) who will install tools for you and mechanisms for you to install tools yourself. Downsides: cluster systems have their own idiosyncrasies and not everyone has access to one. AWS provides large amounts of compute resources and interfaces very well with docker and with S3 for convenient and efficient data storage. Downsides: costs money for each use ; has its own learning curve.","title":"Supported Environments"},{"location":"Supported-Environments/#what-environements-are-supported","text":"The main BioLockJ program can be used in these environments: a local machine with a unix-like system any machine running docker * a cluster, running a scheduler like torque AWS cloud computing * (* The launch scripts will still be run from your local machine.) AWS is currently experimental. To ty it, see Working in Pure Docker In all cases, the launch process requires a unix-like environment. This includes linux, macOS, or an ubuntu environment running on Windows. There is also the option to run purely in docker, without installing even the launch scripts on your local machine. However this is considered a nitch case scenario and not well supported.","title":"What environements are supported"},{"location":"Supported-Environments/#choosing-an-environment","text":"The major resources that come together in a pipeline are: data (project data and reference data) compute resources (mem, ram, cpu) key executables In theory, you could install all the tools you need on your laptop; put your data on your laptop, and run your whole analysis on your laptop. This would be a \"local\" pipeline; a single compute node is handling everything. However, in practice, a single machine typically doesn't have enough compute resources to run a modern bioinformatics pipeline in a realistic time frame; and the tools may be difficult to install, or even impossible to install on a given system. Docker provides key executables by packaging them into containers. After the initial hurdle of installing docker itself, the 'install' of executables that are available in docker images is trivial, and they produce very consistent results; even when different steps in your pipeline have conflicting system requirements. The underlying tools for all modules packaged with the main BioLockJ program are available via docker containers. Docker is the most recommended way to run a pipeline. However, these executables still have to come together with some compute resources. A computer cluster offers large amounts of compute resources and plenty of storage. Some clusters also have adminstrators (or other users) who will install tools for you and mechanisms for you to install tools yourself. Downsides: cluster systems have their own idiosyncrasies and not everyone has access to one. AWS provides large amounts of compute resources and interfaces very well with docker and with S3 for convenient and efficient data storage. Downsides: costs money for each use ; has its own learning curve.","title":"Choosing an environment"},{"location":"The-Metadata-File/","text":"The config file gives information about the pipeline , sometimes specific to one module . The metadata file gives information specific to each sample . This might include membership in sample groups for statistical tests, or barcode sequences for demultiplexing or the filenames from the input dirs to associate with each sample.","title":"Metadata"},{"location":"Validation/","text":"Summary Description: Validation checks whether the output files of a pipeline match the expectation . Options: validation.compareOn validation.disableValidation validation.expectationFile validation.reportOn validation.sizeWithinPercent validation.stopPipeline The validation utility creates a table for the output of each module where it reports the file name, size and md5. These tables are saved in the validation folder; the validation folder generated by a pipeline can be used as the expectations when re-running the same pipeline. If there are no expectations, these values are reported in the validation folder. If there are expectations, these values are reported and compared against the expected values; the result of the comparison is reported as either PASS or FAIL for each file. If validation.stopPipeline=Y , the validation utility will halt the pipeline if any outputs FAIL to meet expectations, otherwise the result is reported and the pipeline moves forward. Soft Validation Many components of a pipeline have the potential for tiny variation: maybe a date is stored in the output, or a reported confidence level is based on a random sampling. With these tiny variations, the file is practically the same, but it will FAIL md5 validation. The file might also be a few bytes bigger or smaller, so it will also FAIL size validation. \"Soft validation\" is the practice of allowing some wiggle room. If the config file gives validation.sizeWithinPercent=1 , then an output file will PASS size validation as long as it is within 1.0% of the expected file size. By default, this value is 0, and a file must be exactly the expected size to pass size validation. Expectations Give the file path to the expectation file using validation.expectationFile=/path/to/saved/validation . This path can either point to a tab-delimited table giving the expectations for a single module, or it can point to a folder, in which case BioLockJ assumes that a file within this folder has a name that matches the module being validated. When validating an entire pipeline, the expectation file for all modules can be passed with a single file path. The validation folder created by a pipeline is designed to be used as this input. The expectation file format is: The expectation file is a tab-delimited table. The first row is column names. The first column (labeled \"name\") gives the file names. Optional column \"size\" gives the file size in bytes. Optional column \"md5\" gives the md5 string. Optional column \"MATCHED_EXPECTION\" is always ignored. The file should not have any other columns. Use cases The expectation is usually based on a previous run of the same pipeline. * Maybe some software has been updated and the results are not expected to change, but you have to re-do your analysis with the latest version to appease reviewers. * Maybe you added a filtering step. * Maybe you just want to change module 5, and you expect 1-4 to produce the same outputs they did last time. * Maybe this analysis has been published and the the original researcher made their pipeline available to you; you want to re-run it and know if the output you generated by running the pipeline is the same as what they had. The expectation can be set by hand. This is recommended for validation using name only, or soft validation using size only. This is a way to prevent a pipeline from continuing after it is effectively doomed. For example: Maybe module 5 is a resource-intensive classifier, and modules 1-4 are processing and filtering steps ending with the SeqFileValidator. If modules 1-4 filter out too much, you might not want to move forward with module 5 until you've made adjustments to the earlier modules. You could create an expectation file for module 4, that just lists the names of the files and their pre-filtering file size (in bytes), and set validation.sizeWithinPercent=80 and SeqFileValidator.stopPipeline=Y . With this, the pipeline will stop if any of those files are not in the module 4 output or if any of them have been reduced by more than 80%. The output file names are predictable if you've ever seen output from that module before.","title":"Validation"},{"location":"Validation/#summary","text":"Description: Validation checks whether the output files of a pipeline match the expectation . Options: validation.compareOn validation.disableValidation validation.expectationFile validation.reportOn validation.sizeWithinPercent validation.stopPipeline The validation utility creates a table for the output of each module where it reports the file name, size and md5. These tables are saved in the validation folder; the validation folder generated by a pipeline can be used as the expectations when re-running the same pipeline. If there are no expectations, these values are reported in the validation folder. If there are expectations, these values are reported and compared against the expected values; the result of the comparison is reported as either PASS or FAIL for each file. If validation.stopPipeline=Y , the validation utility will halt the pipeline if any outputs FAIL to meet expectations, otherwise the result is reported and the pipeline moves forward.","title":"Summary"},{"location":"Validation/#soft-validation","text":"Many components of a pipeline have the potential for tiny variation: maybe a date is stored in the output, or a reported confidence level is based on a random sampling. With these tiny variations, the file is practically the same, but it will FAIL md5 validation. The file might also be a few bytes bigger or smaller, so it will also FAIL size validation. \"Soft validation\" is the practice of allowing some wiggle room. If the config file gives validation.sizeWithinPercent=1 , then an output file will PASS size validation as long as it is within 1.0% of the expected file size. By default, this value is 0, and a file must be exactly the expected size to pass size validation.","title":"Soft Validation"},{"location":"Validation/#expectations","text":"Give the file path to the expectation file using validation.expectationFile=/path/to/saved/validation . This path can either point to a tab-delimited table giving the expectations for a single module, or it can point to a folder, in which case BioLockJ assumes that a file within this folder has a name that matches the module being validated. When validating an entire pipeline, the expectation file for all modules can be passed with a single file path. The validation folder created by a pipeline is designed to be used as this input. The expectation file format is: The expectation file is a tab-delimited table. The first row is column names. The first column (labeled \"name\") gives the file names. Optional column \"size\" gives the file size in bytes. Optional column \"md5\" gives the md5 string. Optional column \"MATCHED_EXPECTION\" is always ignored. The file should not have any other columns.","title":"Expectations"},{"location":"Validation/#use-cases","text":"The expectation is usually based on a previous run of the same pipeline. * Maybe some software has been updated and the results are not expected to change, but you have to re-do your analysis with the latest version to appease reviewers. * Maybe you added a filtering step. * Maybe you just want to change module 5, and you expect 1-4 to produce the same outputs they did last time. * Maybe this analysis has been published and the the original researcher made their pipeline available to you; you want to re-run it and know if the output you generated by running the pipeline is the same as what they had. The expectation can be set by hand. This is recommended for validation using name only, or soft validation using size only. This is a way to prevent a pipeline from continuing after it is effectively doomed. For example: Maybe module 5 is a resource-intensive classifier, and modules 1-4 are processing and filtering steps ending with the SeqFileValidator. If modules 1-4 filter out too much, you might not want to move forward with module 5 until you've made adjustments to the earlier modules. You could create an expectation file for module 4, that just lists the names of the files and their pre-filtering file size (in bytes), and set validation.sizeWithinPercent=80 and SeqFileValidator.stopPipeline=Y . With this, the pipeline will stop if any of those files are not in the module 4 output or if any of them have been reduced by more than 80%. The output file names are predictable if you've ever seen output from that module before.","title":"Use cases"},{"location":"help-biolockj/","text":"The biolockj help menu: biolockj --help BioLockJ v1.2.6-dev - UNCC Fodor Lab July 2018 Usage: biolockj [options] <config|pipeline> Options: -v --version Show version -h --help Show help menu -p --precheck-only Set up pipeline and check dependencies and then STOP; do not execute the pipeline. This is helpful when testing edits to config files. -r --restart Resume an existing pipeline -c --config-override <file> New config file (if restarting a pipeline) --password <password> Encrypt password -d --docker Run in docker -a --aws Run on aws -g --gui Start the BioLockJ GUI -f --foreground Run the java process in the foreground without nohup -w --wait-for-start Do not release terminal until pipeline completes check-dependencies step. --external-modules <dir> Directory with compiled java code giving additional modules --blj Map $BLJ folder into the docker container; this replaces BioLockJ packaged in a docker container with the local copy. -e --env-var <var=val> Environment variables to be passed to the BioLockJ environment. Can be a comma-sep list. Values take the form: a=foo,b=bar,c=baz --blj_proj <dir> Directory that contains BioLockJ pipelines. If not supplied, biolockj will use the value of environment variable \"BLJ_PROJ\".","title":"Help biolockj"},{"location":"module/classifier/module.classifier/","text":"Classifier Package Modules in the biolockj.module.classifier package categorize micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms. This package contains 2 sub-packages: module.classifier.r16s contains modules designed to classify 16S data. module.classifier.wgs contains modules designed to classify whole genome sequence data. Modules in these sub-packages extend the ClassifierModuleImpl class. ClassifierModuleImpl Description: Abstract implementation of the ClassifierModule interface that the other classifier modules extend to inherit standard functionality. Abstract modules cannot be included in the pipeline run order.","title":"Classifier Package"},{"location":"module/classifier/module.classifier/#classifier-package","text":"Modules in the biolockj.module.classifier package categorize micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms. This package contains 2 sub-packages: module.classifier.r16s contains modules designed to classify 16S data. module.classifier.wgs contains modules designed to classify whole genome sequence data. Modules in these sub-packages extend the ClassifierModuleImpl class.","title":"Classifier Package"},{"location":"module/classifier/module.classifier/#classifiermoduleimpl","text":"Description: Abstract implementation of the ClassifierModule interface that the other classifier modules extend to inherit standard functionality. Abstract modules cannot be included in the pipeline run order.","title":"ClassifierModuleImpl"},{"location":"module/classifier/module.classifier.r16s/","text":"biolockj.module.classifier.r16s is a sub-package of module.classifier. Package modules extend ClassifierModuleImpl to cluster and classify 16S micbrobial samples for taxonomy assignment. QiimeClosedRefClassifier #BioModule biolockj.module.classifier.r16s.QiimeClosedRefClassifier Description: This module picks OTUs using a closed reference database and constructs an OTU table via the QIIME script pick_closed_reference_otus.py . Taxonomy is assigned using a pre-defined taxonomy map of reference sequence OTU to taxonomy. This is the fastest OTU picking method since samples can be processed in parallel batches. Before the QIIME script is run, batches are prepared in the temp directory, with each batch directory containing a fasta directory with script.batchSize fasta files and a QIIME mapping file, created with awk, called batchMapping.tsv for the batch of samples. Inherits from QiimeClassifier . Options: exe.awk QiimeDeNovoClassifier #BioModule biolockj.module.classifier.r16s.QiimeDeNovoClassifier Description: This module runs the QIIME pick_de_novo_otus.py script on all fasta sequence files in a single script since OTUs are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras QiimeOpenRefClassifier #BioModule biolockj.module.classifier.r16s.QiimeOpenRefClassifier Description: This module runs the QIIME pick_open_reference_otus.py script on all fasta sequence files in a single script since clusters not identified in the reference database are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras RdpClassifier #BioModule biolockj.module.classifier.r16s.RdpClassifier Description: Classify 16s samples with RDP . Options: exe.java rdp.db rdp.jar rdp.minThresholdScore See also: Typical QIIME Pipeline","title":"Module.classifier.r16s"},{"location":"module/classifier/module.classifier.r16s/#qiimeclosedrefclassifier","text":"#BioModule biolockj.module.classifier.r16s.QiimeClosedRefClassifier Description: This module picks OTUs using a closed reference database and constructs an OTU table via the QIIME script pick_closed_reference_otus.py . Taxonomy is assigned using a pre-defined taxonomy map of reference sequence OTU to taxonomy. This is the fastest OTU picking method since samples can be processed in parallel batches. Before the QIIME script is run, batches are prepared in the temp directory, with each batch directory containing a fasta directory with script.batchSize fasta files and a QIIME mapping file, created with awk, called batchMapping.tsv for the batch of samples. Inherits from QiimeClassifier . Options: exe.awk","title":"QiimeClosedRefClassifier"},{"location":"module/classifier/module.classifier.r16s/#qiimedenovoclassifier","text":"#BioModule biolockj.module.classifier.r16s.QiimeDeNovoClassifier Description: This module runs the QIIME pick_de_novo_otus.py script on all fasta sequence files in a single script since OTUs are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras","title":"QiimeDeNovoClassifier"},{"location":"module/classifier/module.classifier.r16s/#qiimeopenrefclassifier","text":"#BioModule biolockj.module.classifier.r16s.QiimeOpenRefClassifier Description: This module runs the QIIME pick_open_reference_otus.py script on all fasta sequence files in a single script since clusters not identified in the reference database are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras","title":"QiimeOpenRefClassifier"},{"location":"module/classifier/module.classifier.r16s/#rdpclassifier","text":"#BioModule biolockj.module.classifier.r16s.RdpClassifier Description: Classify 16s samples with RDP . Options: exe.java rdp.db rdp.jar rdp.minThresholdScore See also: Typical QIIME Pipeline","title":"RdpClassifier"},{"location":"module/classifier/module.classifier.wgs/","text":"Whole Genome Sequence Classifiers biolockj.module.classifier.wgs is a sub-package of module.classifier. Package modules categorize whole genome sequence micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms. Humann2Classifier #BioModule biolockj.module.classifier.wgs.Humann2Classifier Description: Use the Biobakery HumanN2 program to generate the HMP Unified Metabolic Analysis Network. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies humann2.nuclDB humann2.protDB KrakenClassifier #BioModule biolockj.module.classifier.wgs.KrakenClassifier Description: Classify WGS samples with KRAKEN . Options: kraken.db Kraken2Classifier #BioModule biolockj.module.classifier.wgs.Kraken2Classifier Description: Classify WGS samples with KRAKEN 2 . Options: kraken2.db Metaphlan2Classifier #BioModule biolockj.module.classifier.wgs.Metaphlan2Classifier Description: Classify WGS samples with MetaPhlAn . Options: exe.python metaphlan2.db metaphlan2.mpa_pkl","title":"Whole Genome Sequence Classifiers"},{"location":"module/classifier/module.classifier.wgs/#whole-genome-sequence-classifiers","text":"biolockj.module.classifier.wgs is a sub-package of module.classifier. Package modules categorize whole genome sequence micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms.","title":"Whole Genome Sequence Classifiers"},{"location":"module/classifier/module.classifier.wgs/#humann2classifier","text":"#BioModule biolockj.module.classifier.wgs.Humann2Classifier Description: Use the Biobakery HumanN2 program to generate the HMP Unified Metabolic Analysis Network. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies humann2.nuclDB humann2.protDB","title":"Humann2Classifier"},{"location":"module/classifier/module.classifier.wgs/#krakenclassifier","text":"#BioModule biolockj.module.classifier.wgs.KrakenClassifier Description: Classify WGS samples with KRAKEN . Options: kraken.db","title":"KrakenClassifier"},{"location":"module/classifier/module.classifier.wgs/#kraken2classifier","text":"#BioModule biolockj.module.classifier.wgs.Kraken2Classifier Description: Classify WGS samples with KRAKEN 2 . Options: kraken2.db","title":"Kraken2Classifier"},{"location":"module/classifier/module.classifier.wgs/#metaphlan2classifier","text":"#BioModule biolockj.module.classifier.wgs.Metaphlan2Classifier Description: Classify WGS samples with MetaPhlAn . Options: exe.python metaphlan2.db metaphlan2.mpa_pkl","title":"Metaphlan2Classifier"},{"location":"module/diy/module.diy/","text":"Do It Yourself - DIY Package DIY package The DIY package allows users to customize BioLockJ. GenMod #BioModule biolockj.module.diy.GenMod Description: Allows user to add their own scripts into the BioLockJ pipeline. Options: genMod.launcher genMod.param genMod.scriptPath genMod.dockerContainerName The specified script is executed using the modules script directory as the current working directory. A scriptPath is required. If specified, the launcher program (ie R, Python) will be used. If specified, any param will be listed as arguments to the script. If running in docker, dockerContainerName is required. This is ideal for: Custom analysis for a given pipeline, such as an R or python script Any steps where an appropriate BioLockJ module does not exist Any step in your analysis process that might otherwise have to be done manually can be stored as a custom script so that the entire process is as reproducible as possible. It is SCRONGLY encouraged that users write scripts using common module conventions: use relative file paths (starting with . or .. ) put all generated output in the modules output directory ( ../output ) put any temporary files in the modules temp directory ( ../tmep ). the main pipeline directory would be ../.. , and the output of a previous module such as PearMergedReads would be in ../../*_PearMergedReads/output To use the GenMod module multiple times in a single pipeline, use the AS keyword to direct properties to the correct instance of the module. For example: #BioModule biolockj.module.diy.GenMod AS Part1 #<other modules> #BioModule biolockj.module.diy.GenMod AS Part2 Part1.launcher=python Part1.script=path/to/first/script.py Part2.script=path/to/bash/script/doLast.sh With this, script.py will be run using python. Then other modules will run. Then doLast.sh will be run using the default system (probably bash, unless it has a shebang line specifiying something else).","title":"Do It Yourself - DIY Package"},{"location":"module/diy/module.diy/#do-it-yourself-diy-package","text":"","title":"Do It Yourself - DIY Package"},{"location":"module/diy/module.diy/#diy-package","text":"The DIY package allows users to customize BioLockJ.","title":"DIY package"},{"location":"module/diy/module.diy/#genmod","text":"#BioModule biolockj.module.diy.GenMod Description: Allows user to add their own scripts into the BioLockJ pipeline. Options: genMod.launcher genMod.param genMod.scriptPath genMod.dockerContainerName The specified script is executed using the modules script directory as the current working directory. A scriptPath is required. If specified, the launcher program (ie R, Python) will be used. If specified, any param will be listed as arguments to the script. If running in docker, dockerContainerName is required. This is ideal for: Custom analysis for a given pipeline, such as an R or python script Any steps where an appropriate BioLockJ module does not exist Any step in your analysis process that might otherwise have to be done manually can be stored as a custom script so that the entire process is as reproducible as possible. It is SCRONGLY encouraged that users write scripts using common module conventions: use relative file paths (starting with . or .. ) put all generated output in the modules output directory ( ../output ) put any temporary files in the modules temp directory ( ../tmep ). the main pipeline directory would be ../.. , and the output of a previous module such as PearMergedReads would be in ../../*_PearMergedReads/output To use the GenMod module multiple times in a single pipeline, use the AS keyword to direct properties to the correct instance of the module. For example: #BioModule biolockj.module.diy.GenMod AS Part1 #<other modules> #BioModule biolockj.module.diy.GenMod AS Part2 Part1.launcher=python Part1.script=path/to/first/script.py Part2.script=path/to/bash/script/doLast.sh With this, script.py will be run using python. Then other modules will run. Then doLast.sh will be run using the default system (probably bash, unless it has a shebang line specifiying something else).","title":"GenMod"},{"location":"module/implicit/module.implicit/","text":"biolockj.module.implicit modules are added to BioLockJ pipelines automatically if needed. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules=Y This package contains the following sub-packages: module.implicit.parser contains ParserModule interface & ParserModuleImpl superclass. module.implicit.parser.r16s contains 16S parser modules. module.implicit.parser.wgs contains WGS parser modules. module.implicit.qiime contains QIIME Script wrappers. Demultiplexer (added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Demultiplex samples into separate files for each sample. Options: demultiplexer.barcodeCutoff demultiplexer.barcodeRevComp demultiplexer.strategy metadata.filePath ImportMetadata (added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Required 1st module in every pipeline. If metadata.filePath is undefined, a new metadata file will be created with only a single column \"SAMPLE_ID\". The imported file is converted to required BioLockJ metadata format: tab-delimited, with unique column headers, and empty cells are now populated with metadata.nullValue or \"NA\" if undefined. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue RegisterNumReads (added by BioLockJ) #BioModule biolockj.module.implicit.RegisterNumReads Description: Add \"Num_Reads\" column to metadata file to document the total number of reads per sample. Options: report.numReads","title":"Module.implicit"},{"location":"module/implicit/module.implicit/#demultiplexer","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Demultiplex samples into separate files for each sample. Options: demultiplexer.barcodeCutoff demultiplexer.barcodeRevComp demultiplexer.strategy metadata.filePath","title":"Demultiplexer"},{"location":"module/implicit/module.implicit/#importmetadata","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Required 1st module in every pipeline. If metadata.filePath is undefined, a new metadata file will be created with only a single column \"SAMPLE_ID\". The imported file is converted to required BioLockJ metadata format: tab-delimited, with unique column headers, and empty cells are now populated with metadata.nullValue or \"NA\" if undefined. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue","title":"ImportMetadata"},{"location":"module/implicit/module.implicit/#registernumreads","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.RegisterNumReads Description: Add \"Num_Reads\" column to metadata file to document the total number of reads per sample. Options: report.numReads","title":"RegisterNumReads"},{"location":"module/implicit/module.implicit.parser/","text":"biolockj.module.implicit.parser modules parse classifier output to generate OTU tables. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. This package contains the following sub-packages: module.implicit.parser.r16s modules parse module.classifier.r16s reports. module.implicit.parser.wgs modules parse module.classifier.wgs reports. ParserModuleImpl cannot be included in the pipeline run order Description: Abstract implementation of ParserModule that the other modules extend to inherit standard functionality. Abstract modules cannot be added to a pipeline, but the r16s & WGS sub-packages contain modules that inherit standard parser functionality from this class. Options: report.numHits","title":"Module.implicit.parser"},{"location":"module/implicit/module.implicit.parser/#parsermoduleimpl","text":"cannot be included in the pipeline run order Description: Abstract implementation of ParserModule that the other modules extend to inherit standard functionality. Abstract modules cannot be added to a pipeline, but the r16s & WGS sub-packages contain modules that inherit standard parser functionality from this class. Options: report.numHits","title":"ParserModuleImpl"},{"location":"module/implicit/module.implicit.parser.r16s/","text":"biolockj.module.implicit.parser.r16s is a sub package of module.implicit.parser. Package modules extend ParserModuleImpl to generate OTU tables from 16S classifier output. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. RdpParser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.RdpParser Description: Build OTU tables from RDP reports. Options: rdp.minThresholdScore QiimeParser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.QiimeParser Description: Build OTU tables from QIIME summarize_taxa.py otu_table text file reports. Options: none","title":"Module.implicit.parser.r16s"},{"location":"module/implicit/module.implicit.parser.r16s/#rdpparser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.RdpParser Description: Build OTU tables from RDP reports. Options: rdp.minThresholdScore","title":"RdpParser"},{"location":"module/implicit/module.implicit.parser.r16s/#qiimeparser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.QiimeParser Description: Build OTU tables from QIIME summarize_taxa.py otu_table text file reports. Options: none","title":"QiimeParser"},{"location":"module/implicit/module.implicit.parser.wgs/","text":"biolockj.module.implicit.parser.wgs is a sub package of module.implicit.parser. Package modules extend ParserModuleImpl to generate OTU tables from WGS classifier output. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. Humann2Parser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Humann2Parser Description: Build OTU tables from HumanN2 classifier module output. Options: none KrakenParser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.KrakenParser Description: Build OTU tables from KRAKEN mpa-format reports. Options: none Kraken2Parser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Kraken2Parser Description: Build OTU tables from KRAKEN 2 mpa-format reports. Options: none Metaphlan2Parser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.MetaphlanParser Description: Build OTU tables from Metaphlan2 classifier module reports. Options: none","title":"Module.implicit.parser.wgs"},{"location":"module/implicit/module.implicit.parser.wgs/#humann2parser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Humann2Parser Description: Build OTU tables from HumanN2 classifier module output. Options: none","title":"Humann2Parser"},{"location":"module/implicit/module.implicit.parser.wgs/#krakenparser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.KrakenParser Description: Build OTU tables from KRAKEN mpa-format reports. Options: none","title":"KrakenParser"},{"location":"module/implicit/module.implicit.parser.wgs/#kraken2parser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Kraken2Parser Description: Build OTU tables from KRAKEN 2 mpa-format reports. Options: none","title":"Kraken2Parser"},{"location":"module/implicit/module.implicit.parser.wgs/#metaphlan2parser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.MetaphlanParser Description: Build OTU tables from Metaphlan2 classifier module reports. Options: none","title":"Metaphlan2Parser"},{"location":"module/implicit/module.implicit.qiime/","text":"biolockj.module.implicit.qiime modules are QIIME Script wrappers implicitly added (if needed). Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. BuildQiimeMapping (added by BioLockJ) #BioModule biolockj.module.implicit.qiime.BuildQiimeMapping Description: This module builds a QIIME mapping file from the metadata. If the metadata file contains the correct columns out of order, awk is used to correct the column order. The updated mapping file is verified with the QIIME script validate_mapping_file.py Options: exe.awk QiimeClassifier (added by BioLockJ) #BioModule biolockj.module.implicit.qiime.QiimeClassifier Description: Generates bash script lines to summarize QIIME results, build taxonomy reports, and add alpha diversity metrics (if configured). For a complete list of available metrics, see: http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html Options: qiime.alphaMetrics qiime.pynastAlignDB qiime.refSeqDB qiime.removeChimeras qiime.taxaDB MergeQiimeOtuTables (added by BioLockJ) #BioModule biolockj.module.implicit.qiime.MergeQiimeOtuTables Description: This module runs the QIIME script merge_otu_tables.py to combine the multiple otu_table.biom files output by its required prerequisite module QiimeClosedRefClassifier , so is only necessary if #samples > script.batchSize . Options: none","title":"Module.implicit.qiime"},{"location":"module/implicit/module.implicit.qiime/#buildqiimemapping","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.qiime.BuildQiimeMapping Description: This module builds a QIIME mapping file from the metadata. If the metadata file contains the correct columns out of order, awk is used to correct the column order. The updated mapping file is verified with the QIIME script validate_mapping_file.py Options: exe.awk","title":"BuildQiimeMapping"},{"location":"module/implicit/module.implicit.qiime/#qiimeclassifier","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.qiime.QiimeClassifier Description: Generates bash script lines to summarize QIIME results, build taxonomy reports, and add alpha diversity metrics (if configured). For a complete list of available metrics, see: http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html Options: qiime.alphaMetrics qiime.pynastAlignDB qiime.refSeqDB qiime.removeChimeras qiime.taxaDB","title":"QiimeClassifier"},{"location":"module/implicit/module.implicit.qiime/#mergeqiimeotutables","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.qiime.MergeQiimeOtuTables Description: This module runs the QIIME script merge_otu_tables.py to combine the multiple otu_table.biom files output by its required prerequisite module QiimeClosedRefClassifier , so is only necessary if #samples > script.batchSize . Options: none","title":"MergeQiimeOtuTables"},{"location":"module/report/module.report.humann2/","text":"Pathway Modules Modules in the biolockj.module.report.humann2 sub-package use ParserModule output to produce and process pathway tables, such as those produced by HumanN2 . Humann2CountModule cannot be included in the pipeline run order Description: Abstract class extends JavaModuleImpl that other humann2 classes extend to inherit shared functionality. Abstract modules cannot be included in the pipeline run order. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies AddMetadataToPathwayTables #BioModule biolockj.module.report.humann2.AddMetadataToPathwayTables Description: Add metadata columns to the OTU abundance tables. Options: none RemoveLowPathwayCounts #BioModule biolockj.module.report.humann2.RemoveLowPathwayCounts Description: This BioModule Pathway counts below a configured threshold to zero. These low sample counts are assumed to be miscategorized or genomic contamination. Options: report.minCount RemoveScarcePathwayCounts #BioModule biolockj.module.report.humann2.RemoveScarcePathwayCounts Description: This BioModule removes scarce pathways not found in enough samples. Each pathway must be found in a configurable percentage of samples to be retained. Options: report.scarceCountCutoff","title":"Pathway Modules"},{"location":"module/report/module.report.humann2/#pathway-modules","text":"Modules in the biolockj.module.report.humann2 sub-package use ParserModule output to produce and process pathway tables, such as those produced by HumanN2 .","title":"Pathway Modules"},{"location":"module/report/module.report.humann2/#humann2countmodule","text":"cannot be included in the pipeline run order Description: Abstract class extends JavaModuleImpl that other humann2 classes extend to inherit shared functionality. Abstract modules cannot be included in the pipeline run order. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies","title":"Humann2CountModule"},{"location":"module/report/module.report.humann2/#addmetadatatopathwaytables","text":"#BioModule biolockj.module.report.humann2.AddMetadataToPathwayTables Description: Add metadata columns to the OTU abundance tables. Options: none","title":"AddMetadataToPathwayTables"},{"location":"module/report/module.report.humann2/#removelowpathwaycounts","text":"#BioModule biolockj.module.report.humann2.RemoveLowPathwayCounts Description: This BioModule Pathway counts below a configured threshold to zero. These low sample counts are assumed to be miscategorized or genomic contamination. Options: report.minCount","title":"RemoveLowPathwayCounts"},{"location":"module/report/module.report.humann2/#removescarcepathwaycounts","text":"#BioModule biolockj.module.report.humann2.RemoveScarcePathwayCounts Description: This BioModule removes scarce pathways not found in enough samples. Each pathway must be found in a configurable percentage of samples to be retained. Options: report.scarceCountCutoff","title":"RemoveScarcePathwayCounts"},{"location":"module/report/module.report/","text":"Report Package Modules in the biolockj.module.report package process ParserModule output, merge the OTU tables with the metadata, and can generate various reports and notifications. This package contains the following sub-packages: module.report.otu contains modules designed to produce or process otu tables. module.report.taxa contains modules designed to produce or process taxa tables. module.report.r contains modules that use R to generate statistics and/or visualizations. module.report.humann2 contains modules designed to produce or process pathway tables. Email #BioModule biolockj.module.report.Email Description: Notify user pipeline is complete by emailing out the pipeline summary. Options: mail.encryptedPassword mail.from mail.smtp.auth mail.smtp.host mail.smtp.port mail.smtp.starttls.enable mail.to JsonReport #BioModule biolockj.module.report.JsonReport Description: This module builds a JSON file from the ParserModule output. Options: report.logBase report.taxonomyLevels","title":"Report Package"},{"location":"module/report/module.report/#report-package","text":"Modules in the biolockj.module.report package process ParserModule output, merge the OTU tables with the metadata, and can generate various reports and notifications. This package contains the following sub-packages: module.report.otu contains modules designed to produce or process otu tables. module.report.taxa contains modules designed to produce or process taxa tables. module.report.r contains modules that use R to generate statistics and/or visualizations. module.report.humann2 contains modules designed to produce or process pathway tables.","title":"Report Package"},{"location":"module/report/module.report/#email","text":"#BioModule biolockj.module.report.Email Description: Notify user pipeline is complete by emailing out the pipeline summary. Options: mail.encryptedPassword mail.from mail.smtp.auth mail.smtp.host mail.smtp.port mail.smtp.starttls.enable mail.to","title":"Email"},{"location":"module/report/module.report/#jsonreport","text":"#BioModule biolockj.module.report.JsonReport Description: This module builds a JSON file from the ParserModule output. Options: report.logBase report.taxonomyLevels","title":"JsonReport"},{"location":"module/report/module.report.otu/","text":"OTU report modules Modules in the biolockj.module.report sub-pakcage normalize ParserModule output, merge the OTU tables with the metadata, or process OTU tables. CompileOtuCounts #BioModule biolockj.module.report.otu.CompileOtuCounts Description: Compiles the counts from all OTU count files into a single summary OTU count file containing OTU counts for the entire dataset. Options: none RarefyOtuCounts #BioModule biolockj.module.report.otu.RarefyOtuCounts Description: Applies a mean iterative post-OTU classification rarefication algorithm so that each output sample will have approximately the same number of OTUs. Options: rarefyOtuCounts.iterations rarefyOtuCounts.lowAbundantCutoff rarefyOtuCounts.quantile rarefyOtuCounts.removeSamplesBelowQuantile RemoveLowOtuCounts #BioModule biolockj.module.report.otu.RemoveLowOtuCounts Description: Removes OTUs with counts below report.minCount . Options: report.minCount report.numHits RemoveScarceOtuCounts #BioModule biolockj.module.report.otu.RemoveScarceOtuCounts Description: Removes OTUs that are not found in enough samples. Options: report.scarceCountCutoff","title":"OTU report modules"},{"location":"module/report/module.report.otu/#otu-report-modules","text":"Modules in the biolockj.module.report sub-pakcage normalize ParserModule output, merge the OTU tables with the metadata, or process OTU tables.","title":"OTU report modules"},{"location":"module/report/module.report.otu/#compileotucounts","text":"#BioModule biolockj.module.report.otu.CompileOtuCounts Description: Compiles the counts from all OTU count files into a single summary OTU count file containing OTU counts for the entire dataset. Options: none","title":"CompileOtuCounts"},{"location":"module/report/module.report.otu/#rarefyotucounts","text":"#BioModule biolockj.module.report.otu.RarefyOtuCounts Description: Applies a mean iterative post-OTU classification rarefication algorithm so that each output sample will have approximately the same number of OTUs. Options: rarefyOtuCounts.iterations rarefyOtuCounts.lowAbundantCutoff rarefyOtuCounts.quantile rarefyOtuCounts.removeSamplesBelowQuantile","title":"RarefyOtuCounts"},{"location":"module/report/module.report.otu/#removelowotucounts","text":"#BioModule biolockj.module.report.otu.RemoveLowOtuCounts Description: Removes OTUs with counts below report.minCount . Options: report.minCount report.numHits","title":"RemoveLowOtuCounts"},{"location":"module/report/module.report.otu/#removescarceotucounts","text":"#BioModule biolockj.module.report.otu.RemoveScarceOtuCounts Description: Removes OTUs that are not found in enough samples. Options: report.scarceCountCutoff","title":"RemoveScarceOtuCounts"},{"location":"module/report/module.report.r/","text":"R Report Modules Modules in the biolockj.module.report.r sub-package generate the statistical analysis and visualizations by executing R scripts. The statistical analysis is performed on the taxa abundance tables generated by AddMetadataToTaxaTables. R_Module cannot be included in the pipeline run order Description: Abstract implementation of ScriptModule that other R modules extend to inherit standard R script functionality. Abstract modules cannot be included in the pipeline run order. Options: exe.rScript r.debug r.nominalFields r.numericFields r.rareOtuThreshold r.reportFields r.saveRData r.timeout report.numHits report.numReads report.taxonomyLevel R_CalculateStats #BioModule biolockj.module.report.r.R_CalculateStats Description: Generate a summary statistics table with [adjusted and unadjusted] [parameteric and non-parametirc] p-values and r 2 values for each reportable metadata field and each report.taxonomyLevel configured. Options: r_CalculateStats.pAdjustMethod r_CalculateStats.pAdjustScope R_PlotEffectSize #BioModule biolockj.module.report.r.R_PlotEffectSize Description: Generate horizontal barplot representing effect size (Cohen's d, r 2 , and/or fold change) for each reportable metadata field and each report.taxonomyLevel configured. Options: r_PlotEffectSize.parametricPval r_PlotEffectSize.disablePvalAdj r_PlotEffectSize.excludePvalAbove r_PlotEffectSize.taxa r_PlotEffectSize.maxNumTaxa r_PlotEffectSize.disableCohensD r_PlotEffectSize.disableRSquared r_PlotEffectSize.disableFoldChange r.colorHighlight r.pvalCutoff R_PlotMds #BioModule biolockj.module.report.r.R_PlotMds Description: Generate sets of multidimensional scaling plots showing 2 axes at a time (up to the < r_PlotMds.numAxis >th axis) with color coding based on each categorical metadata field (default) or by each field given in r_PlotMds.reportFields . Options: r_PlotMds.numAxis r_PlotMds.reportFields r_PlotMds.distance r.colorPalette r.colorPoint r.pch r.pvalCutoff r.pValFormat R_PlotOtus #BioModule biolockj.module.report.r.R_PlotOtus Description: Generate OTU-metadata box-plots and scatter-plots for each reportable metadata field and each report.taxonomyLevel configured Options: r.colorBase r.colorHighlight r.colorPalette r.colorPoint r.pch r.pvalCutoff r.rareOtuThreshold r.pValFormat R_PlotPvalHistograms #BioModule biolockj.module.report.r.R_PlotPvalHistograms Description: Generate p-value histograms for each reportable metadata field and each report.taxonomyLevel configured Options: r.pvalCutoff","title":"R Report Modules"},{"location":"module/report/module.report.r/#r-report-modules","text":"Modules in the biolockj.module.report.r sub-package generate the statistical analysis and visualizations by executing R scripts. The statistical analysis is performed on the taxa abundance tables generated by AddMetadataToTaxaTables.","title":"R Report Modules"},{"location":"module/report/module.report.r/#r_module","text":"cannot be included in the pipeline run order Description: Abstract implementation of ScriptModule that other R modules extend to inherit standard R script functionality. Abstract modules cannot be included in the pipeline run order. Options: exe.rScript r.debug r.nominalFields r.numericFields r.rareOtuThreshold r.reportFields r.saveRData r.timeout report.numHits report.numReads report.taxonomyLevel","title":"R_Module"},{"location":"module/report/module.report.r/#r_calculatestats","text":"#BioModule biolockj.module.report.r.R_CalculateStats Description: Generate a summary statistics table with [adjusted and unadjusted] [parameteric and non-parametirc] p-values and r 2 values for each reportable metadata field and each report.taxonomyLevel configured. Options: r_CalculateStats.pAdjustMethod r_CalculateStats.pAdjustScope","title":"R_CalculateStats"},{"location":"module/report/module.report.r/#r_ploteffectsize","text":"#BioModule biolockj.module.report.r.R_PlotEffectSize Description: Generate horizontal barplot representing effect size (Cohen's d, r 2 , and/or fold change) for each reportable metadata field and each report.taxonomyLevel configured. Options: r_PlotEffectSize.parametricPval r_PlotEffectSize.disablePvalAdj r_PlotEffectSize.excludePvalAbove r_PlotEffectSize.taxa r_PlotEffectSize.maxNumTaxa r_PlotEffectSize.disableCohensD r_PlotEffectSize.disableRSquared r_PlotEffectSize.disableFoldChange r.colorHighlight r.pvalCutoff","title":"R_PlotEffectSize"},{"location":"module/report/module.report.r/#r_plotmds","text":"#BioModule biolockj.module.report.r.R_PlotMds Description: Generate sets of multidimensional scaling plots showing 2 axes at a time (up to the < r_PlotMds.numAxis >th axis) with color coding based on each categorical metadata field (default) or by each field given in r_PlotMds.reportFields . Options: r_PlotMds.numAxis r_PlotMds.reportFields r_PlotMds.distance r.colorPalette r.colorPoint r.pch r.pvalCutoff r.pValFormat","title":"R_PlotMds"},{"location":"module/report/module.report.r/#r_plototus","text":"#BioModule biolockj.module.report.r.R_PlotOtus Description: Generate OTU-metadata box-plots and scatter-plots for each reportable metadata field and each report.taxonomyLevel configured Options: r.colorBase r.colorHighlight r.colorPalette r.colorPoint r.pch r.pvalCutoff r.rareOtuThreshold r.pValFormat","title":"R_PlotOtus"},{"location":"module/report/module.report.r/#r_plotpvalhistograms","text":"#BioModule biolockj.module.report.r.R_PlotPvalHistograms Description: Generate p-value histograms for each reportable metadata field and each report.taxonomyLevel configured Options: r.pvalCutoff","title":"R_PlotPvalHistograms"},{"location":"module/report/module.report.taxa/","text":"Modules in the biolockj.module.report.taxa package process ParserModule output to produce or process taxa tables. AddMetadataToTaxaTables #BioModule biolockj.module.report.taxa.AddMetadataToTaxaTables Description: Map metadata onto taxa tables using sample ID. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue report.taxonomyLevels AddPseudoCount #BioModule biolockj.module.report.taxa.AddPseudoCount Description: Add 1.0 to every value in each table. This is typically done to avoid the mathmatecal consequences of 0's, and is generally only done on raw counts data. Options: none BuildTaxaTables #BioModule biolockj.module.report.taxa.BuildTaxaTables Description: Process ParserModule output to produce taxa tables. This module reads the most recent OTU count files generated by any previous BioModule and re-writes the data as separate tables containing taxa counts for each taxonomy level. Options: report.taxonomyLevels LogTransformTaxaTables #BioModule biolockj.module.report.taxa.LogTransformTaxaTables Description: Log-transform the raw taxa counts on Log10 or Log-e scales. Options: report.logBase NormalizeByReadsPerMillion #BioModule biolockj.module.report.taxa.NormalizeByReadsPerMillion Description: Normalize each sample for sequencing depth by reporting each value as the number of counts per million counts in a given sample. Options: none NormalizeTaxaTables #BioModule biolockj.module.report.taxa.NormalizeTaxaTables Description: Normalize taxa tables based on formula: counts_{normalized} = \\frac{counts_{raw}}{n} \\frac{\\sum (x)}{N} +1 Where: counts_{raw} = raw count; the cell value before normalizing n = number of sequences in the sample (total within a sample) \\sum (x) = total number of counts in the table (total across samples) N = total number of samples Typically the data is put on a Log_{10} scale, so the full forumula is: counts_{final} = Log_{10} \\biggl( \\frac{counts_{raw}}{n} \\frac{\\sum (x)}{N} +1 \\biggr) The counts_{final} values will be in output dir of the LogTransformTaxaTables module. The counts_{normalized} values will be in the output of the NormalizeTaxaTables module. For further explanation regarding the normalization scheme, please read The ISME Journal 2013 paper by Dr. Anthony Fodor: \"Stochastic changes over time and not founder effects drive cage effects in microbial community assembly in a mouse model\" If report.logBase is not null, then the LogTransformTaxaTables will be added as a post-requisit module. Options: report.logBase","title":"Module.report.taxa"},{"location":"module/report/module.report.taxa/#addmetadatatotaxatables","text":"#BioModule biolockj.module.report.taxa.AddMetadataToTaxaTables Description: Map metadata onto taxa tables using sample ID. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue report.taxonomyLevels","title":"AddMetadataToTaxaTables"},{"location":"module/report/module.report.taxa/#addpseudocount","text":"#BioModule biolockj.module.report.taxa.AddPseudoCount Description: Add 1.0 to every value in each table. This is typically done to avoid the mathmatecal consequences of 0's, and is generally only done on raw counts data. Options: none","title":"AddPseudoCount"},{"location":"module/report/module.report.taxa/#buildtaxatables","text":"#BioModule biolockj.module.report.taxa.BuildTaxaTables Description: Process ParserModule output to produce taxa tables. This module reads the most recent OTU count files generated by any previous BioModule and re-writes the data as separate tables containing taxa counts for each taxonomy level. Options: report.taxonomyLevels","title":"BuildTaxaTables"},{"location":"module/report/module.report.taxa/#logtransformtaxatables","text":"#BioModule biolockj.module.report.taxa.LogTransformTaxaTables Description: Log-transform the raw taxa counts on Log10 or Log-e scales. Options: report.logBase","title":"LogTransformTaxaTables"},{"location":"module/report/module.report.taxa/#normalizebyreadspermillion","text":"#BioModule biolockj.module.report.taxa.NormalizeByReadsPerMillion Description: Normalize each sample for sequencing depth by reporting each value as the number of counts per million counts in a given sample. Options: none","title":"NormalizeByReadsPerMillion"},{"location":"module/report/module.report.taxa/#normalizetaxatables","text":"#BioModule biolockj.module.report.taxa.NormalizeTaxaTables Description: Normalize taxa tables based on formula: counts_{normalized} = \\frac{counts_{raw}}{n} \\frac{\\sum (x)}{N} +1 Where: counts_{raw} = raw count; the cell value before normalizing n = number of sequences in the sample (total within a sample) \\sum (x) = total number of counts in the table (total across samples) N = total number of samples Typically the data is put on a Log_{10} scale, so the full forumula is: counts_{final} = Log_{10} \\biggl( \\frac{counts_{raw}}{n} \\frac{\\sum (x)}{N} +1 \\biggr) The counts_{final} values will be in output dir of the LogTransformTaxaTables module. The counts_{normalized} values will be in the output of the NormalizeTaxaTables module. For further explanation regarding the normalization scheme, please read The ISME Journal 2013 paper by Dr. Anthony Fodor: \"Stochastic changes over time and not founder effects drive cage effects in microbial community assembly in a mouse model\" If report.logBase is not null, then the LogTransformTaxaTables will be added as a post-requisit module. Options: report.logBase","title":"NormalizeTaxaTables"},{"location":"module/seq/module.seq/","text":"Seq Package Modules from the biolockj.module.seq package prepare sequence data or metadata prior to classification. If included, seq modules must be ordered to run before modules from any of the other packages. AwkFastaConverter #BioModule biolockj.module.seq.AwkFastaConverter Description: Convert fastq files into fasta format (required by QIIME ). Options: exe.awk exe.gzip Gunzipper #BioModule biolockj.module.seq.Gunzipper Description: Decompress gzipped files. Options: exe.gzip KneadData #BioModule biolockj.module.seq.KneadData Description: Runs the Biobakery KneadData program to remove contaminated DNA. Options: kneaddata.dbs exe.kneaddata exe.kneaddataParams Multiplexer #BioModule biolockj.module.seq.Multiplexer Description: Multiplex samples into a single file, or two files (one with forward reads, one with reverse reads) if multiplexing paired reads. BioLockJ modules require demultiplexed data, so if included, this must be the last module in the pipeline other than module.report modules. Options: metadata.barcodeColumn metadata.filePath PearMergeReads #BioModule biolockj.module.seq.PearMergeReads Description: Merge paired reads (required for RDP & QIIME ). For more informations, see the online PEAR manual . Options: exe.pear exe.pearParams RarefySeqs #BioModule biolockj.module.seq.RarefySeqs Description: Randomly select samples to reduce all samples to the configured maximum. Samples with less than the minimum number of reads are discarded. Options: rarefySeqs.max rarefySeqs.min SeqFileValidator #BioModule biolockj.module.seq.SeqFileValidator Description: This BioModule validates fasta/fastq file formats are valid and enforces min/max read lengths. Options: input.seqMaxLen input.seqMinLen TrimPrimers #BioModule biolockj.module.seq.TrimPrimers Description: Remove primers from reads, option to discard reads unless primers are attached to both forward and reverse reads. Options: trimPrimers.filePath trimPrimers.requirePrimer","title":"Seq Package"},{"location":"module/seq/module.seq/#seq-package","text":"Modules from the biolockj.module.seq package prepare sequence data or metadata prior to classification. If included, seq modules must be ordered to run before modules from any of the other packages.","title":"Seq Package"},{"location":"module/seq/module.seq/#awkfastaconverter","text":"#BioModule biolockj.module.seq.AwkFastaConverter Description: Convert fastq files into fasta format (required by QIIME ). Options: exe.awk exe.gzip","title":"AwkFastaConverter"},{"location":"module/seq/module.seq/#gunzipper","text":"#BioModule biolockj.module.seq.Gunzipper Description: Decompress gzipped files. Options: exe.gzip","title":"Gunzipper"},{"location":"module/seq/module.seq/#kneaddata","text":"#BioModule biolockj.module.seq.KneadData Description: Runs the Biobakery KneadData program to remove contaminated DNA. Options: kneaddata.dbs exe.kneaddata exe.kneaddataParams","title":"KneadData"},{"location":"module/seq/module.seq/#multiplexer","text":"#BioModule biolockj.module.seq.Multiplexer Description: Multiplex samples into a single file, or two files (one with forward reads, one with reverse reads) if multiplexing paired reads. BioLockJ modules require demultiplexed data, so if included, this must be the last module in the pipeline other than module.report modules. Options: metadata.barcodeColumn metadata.filePath","title":"Multiplexer"},{"location":"module/seq/module.seq/#pearmergereads","text":"#BioModule biolockj.module.seq.PearMergeReads Description: Merge paired reads (required for RDP & QIIME ). For more informations, see the online PEAR manual . Options: exe.pear exe.pearParams","title":"PearMergeReads"},{"location":"module/seq/module.seq/#rarefyseqs","text":"#BioModule biolockj.module.seq.RarefySeqs Description: Randomly select samples to reduce all samples to the configured maximum. Samples with less than the minimum number of reads are discarded. Options: rarefySeqs.max rarefySeqs.min","title":"RarefySeqs"},{"location":"module/seq/module.seq/#seqfilevalidator","text":"#BioModule biolockj.module.seq.SeqFileValidator Description: This BioModule validates fasta/fastq file formats are valid and enforces min/max read lengths. Options: input.seqMaxLen input.seqMinLen","title":"SeqFileValidator"},{"location":"module/seq/module.seq/#trimprimers","text":"#BioModule biolockj.module.seq.TrimPrimers Description: Remove primers from reads, option to discard reads unless primers are attached to both forward and reverse reads. Options: trimPrimers.filePath trimPrimers.requirePrimer","title":"TrimPrimers"}]}